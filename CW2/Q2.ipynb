{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D, Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# https://github.com/eriklindernoren/Keras-GAN\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "\n",
    "root = 'CGAN_results/'\n",
    "\n",
    "# plotting data structure\n",
    "train_hist = {}\n",
    "train_hist['D_losses'] = []\n",
    "train_hist['G_losses'] = []\n",
    "train_hist['per_epoch_ptimes'] = []\n",
    "train_hist['total_ptime'] = []\n",
    "train_hist['accuracy'] = []\n",
    "train_hist['Model'] = []\n",
    "train_hist['classif_loss'] = []\n",
    "train_hist['classif_acc'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainClassif():\n",
    "    (train_set, train_label_raw), (_, _) = mnist.load_data()\n",
    "    train_label = to_categorical(train_label_raw)\n",
    "    train_data = np.asarray(train_set).reshape(60000, 28, 28, 1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_data, train_label, test_size=0.33, random_state=43)\n",
    "\n",
    "#     test_data = test_set.reshape(10000, 28, 28, 1)\n",
    "\n",
    "#     np.shape(train_data)\n",
    "\n",
    "    model = Sequential()\n",
    "    # model.add(Conv2D(5, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "    model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=8)\n",
    "    return model\n",
    "\n",
    "def getClassif():\n",
    "    model = Sequential()\n",
    "    # model.add(Conv2D(5, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "    model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# classif = trainClassif()\n",
    "# os.makedirs('saved_model_weights/classif/', exist_ok=True)\n",
    "# classif.save_weights('saved_model_weights/classif/pre_trained_classif_short.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classif = getClassif()\n",
    "classif.load_weights('saved_model_weights/classif/pre_trained_classif.h5', by_name=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 184us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08390473165685544, 0.9773]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(_,_), (X_test_raw, y_test_raw) = mnist.load_data()\n",
    "X_test = X_test_raw.reshape(10000,28,28,1)\n",
    "y_test = to_categorical(y_test_raw)\n",
    "classif.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self, pick_model='Deep_BN'):\n",
    "        # Input shape\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.num_classes = 10\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator(pick_model=pick_model)\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator(pick_model=pick_model)\n",
    "\n",
    "        # The generator takes noise and the target label as input\n",
    "        # and generates the corresponding digit of that label\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([noise, label])\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
    "        valid = self.discriminator([img, label])\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model([noise, label], valid)\n",
    "        self.combined.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self, pick_model='Deep_BN'):\n",
    "        if (pick_model == 'Deep_BN') | (pick_model == 'Deeper_G_BN'):\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(256, input_dim=self.latent_dim))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(BatchNormalization(momentum=0.8))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(BatchNormalization(momentum=0.8))\n",
    "            model.add(Dense(1024))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(BatchNormalization(momentum=0.8))\n",
    "            model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "            model.add(Reshape(self.img_shape))\n",
    "\n",
    "            model.summary()\n",
    "\n",
    "        elif (pick_model == 'Shallow_BN') | (pick_model == 'Deeper_D_BN'): \n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(256, input_dim=self.latent_dim))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(BatchNormalization(momentum=0.8))\n",
    "            model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "            model.add(Reshape(self.img_shape))\n",
    "\n",
    "            model.summary()\n",
    "        elif pick_model == 'Shallow_Drop':\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(256, input_dim=self.latent_dim))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dropout(0.4))\n",
    "            model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "            model.add(Reshape(self.img_shape))\n",
    "\n",
    "            model.summary()\n",
    "        elif pick_model == 'Drop':\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(256, input_dim=self.latent_dim))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(BatchNormalization(momentum=0.8))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(BatchNormalization(momentum=0.8))\n",
    "            model.add(Dense(1024))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(BatchNormalization(momentum=0.8))\n",
    "            model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "            model.add(Reshape(self.img_shape))\n",
    "\n",
    "            model.summary()\n",
    "        elif pick_model == 'Shallow_simple':\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(128, input_dim=self.latent_dim))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "#             model.add(BatchNormalization(momentum=0.8))\n",
    "            model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "            model.add(Reshape(self.img_shape))\n",
    "\n",
    "            model.summary()\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "\n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        img = model(model_input)\n",
    "\n",
    "        return Model([noise, label], img)\n",
    "\n",
    "    def build_discriminator(self, pick_model='Deep_BN'):\n",
    "        if (pick_model == 'Deep_BN') | (pick_model == 'Deeper_D_BN'):\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(512, input_dim=np.prod(self.img_shape)))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dropout(0.4))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dropout(0.4))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.summary()\n",
    "        \n",
    "        elif (pick_model == 'Shallow_BN') | (pick_model == 'Deeper_G_BN'): \n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(512, input_dim=np.prod(self.img_shape)))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dropout(0.4))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dropout(0.4))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.summary()\n",
    "        \n",
    "        elif pick_model == 'Shallow_Drop':\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(512, input_dim=np.prod(self.img_shape)))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "            model.add(Dropout(0.4))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.summary()\n",
    "        \n",
    "        elif pick_model == 'Drop':\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(512, input_dim=np.prod(self.img_shape)))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dropout(0.4))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dropout(0.4))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.summary()\n",
    "            \n",
    "        elif pick_model == 'Shallow_simple':\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(128, input_dim=np.prod(self.img_shape)))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.summary()      \n",
    "            \n",
    "        img = Input(shape=self.img_shape)\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
    "        flat_img = Flatten()(img)\n",
    "\n",
    "        model_input = multiply([flat_img, label_embedding])\n",
    "\n",
    "        validity = model(model_input)\n",
    "\n",
    "        return Model([img, label], validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50, ratio = (1,1), pick_model = 'Deep_BN'):\n",
    "        best_acc = 0\n",
    "        # Load the dataset\n",
    "        (X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Configure input\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            epoch_start_time = time.time()\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs, labels = X_train[idx], y_train[idx]\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict([noise, labels])\n",
    "            #implement ratio of D  \n",
    "            if epoch % ratio[0] == 0:\n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs, labels], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, labels], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Condition on labels\n",
    "            sampled_labels = np.random.randint(0, 10, batch_size).reshape(-1, 1)\n",
    "            #implement ratio of G\n",
    "            if epoch % ratio[1] == 0:\n",
    "                # Train the generator\n",
    "                g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "            \n",
    "            # save data for plotting and csv\n",
    "            train_hist['D_losses'].append(d_loss[0])\n",
    "            train_hist['G_losses'].append(g_loss)\n",
    "            train_hist['per_epoch_ptimes'].append(per_epoch_ptime)    \n",
    "            train_hist['accuracy'].append(100 * d_loss[1])\n",
    "            train_hist['Model'].append(pick_model)\n",
    "            \n",
    "            \n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch, pick_model, ratio)\n",
    "                # Plot the progress\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "                # REMEMBER TO CHANGE TO PARAM DIRECTORY\n",
    "                if pick_model == 'Deep_BN':\n",
    "                    gen_images, gen_labels = cgan.generate_images(100)\n",
    "                    loss, acc = classif.evaluate(gen_images, to_categorical(gen_labels))\n",
    "                    train_hist['classif_loss'].append(loss)\n",
    "                    train_hist['classif_acc'].append(acc)\n",
    "                    \n",
    "                    if (acc > 0.92) & (acc > best_acc):\n",
    "                        best_acc = acc\n",
    "                        \n",
    "                        gen_images, gen_labels = cgan.generate_images(10000)\n",
    "                        image_out = gen_images.reshape(100000,784)\n",
    "\n",
    "                        df = pd.DataFrame(image_out)\n",
    "                        df.to_csv('generated_images_%d'%epoch +'_%f.csv'%acc, index=False, header=False)\n",
    "                        dfl = pd.DataFrame(gen_labels)\n",
    "                        dfl.to_csv('generated_labels_%d'%epoch +'_%f.csv'%acc, index=False, header=False)\n",
    "#                 os.makedirs('saved_model_weights/generator_weights/', exist_ok=True)\n",
    "#                 generator.save_weights('saved_model_weights/generator_weights/' + pick_model + '_' + ratio + '_' + epoch + '.h5')\n",
    "\n",
    "#             if epoch == range(epochs):    \n",
    "#                 os.makedirs('saved_model_weights/generator_weights/', exist_ok=True)\n",
    "#                 generator.save_weights('saved_model_weights/generator_weights/' + pick_model + '_' + ratio + '_' + epoch + '.h5')\n",
    "\n",
    "#                 os.makedirs('saved_model_weights', exist_ok=True)\n",
    "#                 self.generator.save_weights('saved_model_weights/generator_weights_%d.h5'%epoch)\n",
    "#                 discriminator.save_weights('saved_model_weights/discriminator_weights_%d.h5'%epoch)\n",
    "#                 combined.save_weights('saved_model_weights/combined_weights-%d.h5'%epoch)\n",
    "\n",
    "    def sample_images(self, epoch, pick_model, ratio):\n",
    "        r, c = 2, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        sampled_labels = np.arange(0, 10).reshape(-1, 1)\n",
    "\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
    "                axs[i,j].set_title(\"Digit: %d\" % sampled_labels[cnt])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        os.makedirs(root + 'images/'+  pick_model + '/'+ str(ratio[0]) + '_' + str(ratio[1]) + '/' , exist_ok=True)\n",
    "        fig.savefig(root + 'images/'+  pick_model + '/'+ str(ratio[0]) + '_' + str(ratio[1]) + '/' + str(epoch) + '.png')\n",
    "#         fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()\n",
    "        \n",
    "    def generate_images(self, num):\n",
    "        r, c = 2, 5\n",
    "        arr_imgs = []\n",
    "        arr_labels = []\n",
    "        for i in range(0, num):\n",
    "            noise = np.random.normal(0, 1, (2 * 5, 100))\n",
    "            sampled_labels = np.arange(0, 10).reshape(-1, 1)\n",
    "            gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "\n",
    "            # Rescale images 0 - 1\n",
    "            gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "            gen_images = ((gen_imgs.astype(np.float32) * 255)) \n",
    "\n",
    "            arr_imgs.append(gen_images)\n",
    "            arr_labels.append(sampled_labels)\n",
    "            \n",
    "        out_imgs = np.asarray(arr_imgs).reshape(num*10, 28, 28, 1)\n",
    "        out_labels = np.asarray(arr_labels).reshape(num*10, 1)\n",
    "        \n",
    "        return out_imgs, out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
    "    x = range(len(hist['D_losses']))\n",
    "    \n",
    "    y1 = hist['D_losses']\n",
    "    y2 = hist['G_losses']\n",
    "\n",
    "    plt.plot(x, y1, label='D_loss')\n",
    "    plt.plot(x, y2, label='G_loss')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    if save:\n",
    "        \n",
    "        plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 927,745\n",
      "Trainable params: 927,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ryan\\uni\\fyp\\venv3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.701835, acc.: 4.69%] [G loss: 0.690731]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "100 [D loss: 0.001938, acc.: 100.00%] [G loss: 7.635430]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "200 [D loss: 0.139606, acc.: 96.88%] [G loss: 8.153555]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "300 [D loss: 0.108064, acc.: 96.88%] [G loss: 7.698699]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "400 [D loss: 0.106099, acc.: 97.66%] [G loss: 6.248728]\n",
      "1000/1000 [==============================] - 0s 125us/step\n",
      "500 [D loss: 0.166373, acc.: 92.19%] [G loss: 5.759964]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "600 [D loss: 0.285154, acc.: 86.72%] [G loss: 5.379868]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "700 [D loss: 0.231625, acc.: 90.62%] [G loss: 4.400356]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "800 [D loss: 0.239354, acc.: 90.62%] [G loss: 4.846135]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "900 [D loss: 0.157206, acc.: 95.31%] [G loss: 4.325408]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "1000 [D loss: 0.146343, acc.: 94.53%] [G loss: 3.956615]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "1100 [D loss: 0.200994, acc.: 92.19%] [G loss: 3.745795]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "1200 [D loss: 0.222909, acc.: 91.41%] [G loss: 4.353328]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "1300 [D loss: 0.149659, acc.: 96.88%] [G loss: 4.122696]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "1400 [D loss: 0.209344, acc.: 90.62%] [G loss: 3.937818]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "1500 [D loss: 0.182285, acc.: 93.75%] [G loss: 3.602478]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "1600 [D loss: 0.139717, acc.: 96.88%] [G loss: 3.818095]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "1700 [D loss: 0.212925, acc.: 90.62%] [G loss: 3.765951]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "1800 [D loss: 0.159615, acc.: 90.62%] [G loss: 3.706004]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "1900 [D loss: 0.313895, acc.: 88.28%] [G loss: 3.843212]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "2000 [D loss: 0.308601, acc.: 87.50%] [G loss: 3.985220]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "2100 [D loss: 0.237273, acc.: 90.62%] [G loss: 3.843634]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "2200 [D loss: 0.461162, acc.: 77.34%] [G loss: 3.063607]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "2300 [D loss: 0.335160, acc.: 85.16%] [G loss: 4.105477]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "2400 [D loss: 0.277828, acc.: 86.72%] [G loss: 3.798596]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "2500 [D loss: 0.292864, acc.: 85.94%] [G loss: 3.756649]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "2600 [D loss: 0.306861, acc.: 87.50%] [G loss: 3.559258]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "2700 [D loss: 0.373578, acc.: 82.03%] [G loss: 2.759789]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "2800 [D loss: 0.225971, acc.: 90.62%] [G loss: 3.425754]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "2900 [D loss: 0.367328, acc.: 82.81%] [G loss: 3.251044]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "3000 [D loss: 0.341620, acc.: 88.28%] [G loss: 2.762613]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "3100 [D loss: 0.250366, acc.: 89.84%] [G loss: 2.592594]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "3200 [D loss: 0.389458, acc.: 84.38%] [G loss: 2.942683]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "3300 [D loss: 0.341716, acc.: 82.03%] [G loss: 3.392316]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "3400 [D loss: 0.353233, acc.: 87.50%] [G loss: 2.972119]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "3500 [D loss: 0.327049, acc.: 87.50%] [G loss: 2.880384]\n",
      "1000/1000 [==============================] - 0s 63us/step\n",
      "3600 [D loss: 0.367505, acc.: 84.38%] [G loss: 2.714934]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "3700 [D loss: 0.326425, acc.: 83.59%] [G loss: 2.919066]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "3800 [D loss: 0.313116, acc.: 86.72%] [G loss: 2.766410]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "3900 [D loss: 0.286522, acc.: 89.84%] [G loss: 2.758058]\n",
      "1000/1000 [==============================] - 0s 63us/step\n",
      "4000 [D loss: 0.335873, acc.: 84.38%] [G loss: 2.399948]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "4100 [D loss: 0.370468, acc.: 82.03%] [G loss: 2.975688]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "4200 [D loss: 0.306647, acc.: 85.16%] [G loss: 2.730964]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "4300 [D loss: 0.464741, acc.: 76.56%] [G loss: 2.887057]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "4400 [D loss: 0.346388, acc.: 86.72%] [G loss: 2.393591]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "4500 [D loss: 0.352274, acc.: 82.03%] [G loss: 2.698896]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "4600 [D loss: 0.353773, acc.: 83.59%] [G loss: 2.695055]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "4700 [D loss: 0.376164, acc.: 81.25%] [G loss: 2.190160]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "4800 [D loss: 0.404858, acc.: 78.91%] [G loss: 2.405217]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "4900 [D loss: 0.407221, acc.: 81.25%] [G loss: 2.199742]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "5000 [D loss: 0.315551, acc.: 85.16%] [G loss: 2.364638]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "5100 [D loss: 0.401268, acc.: 80.47%] [G loss: 2.251716]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "5200 [D loss: 0.475504, acc.: 75.00%] [G loss: 2.107480]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "5300 [D loss: 0.330288, acc.: 84.38%] [G loss: 2.565176]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "5400 [D loss: 0.383301, acc.: 80.47%] [G loss: 2.621175]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "5500 [D loss: 0.385717, acc.: 82.81%] [G loss: 2.276204]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "5600 [D loss: 0.355555, acc.: 85.94%] [G loss: 2.358652]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "5700 [D loss: 0.398794, acc.: 79.69%] [G loss: 2.116909]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "5800 [D loss: 0.509767, acc.: 70.31%] [G loss: 2.101111]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "5900 [D loss: 0.394481, acc.: 85.16%] [G loss: 2.195488]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "6000 [D loss: 0.296546, acc.: 86.72%] [G loss: 2.378913]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "6100 [D loss: 0.391440, acc.: 82.81%] [G loss: 2.345637]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "6200 [D loss: 0.391382, acc.: 81.25%] [G loss: 2.206856]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "6300 [D loss: 0.391610, acc.: 84.38%] [G loss: 2.057855]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "6400 [D loss: 0.399258, acc.: 80.47%] [G loss: 2.506576]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "6500 [D loss: 0.492235, acc.: 75.78%] [G loss: 2.415029]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "6600 [D loss: 0.408763, acc.: 82.03%] [G loss: 2.302143]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "6700 [D loss: 0.413180, acc.: 83.59%] [G loss: 2.327110]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "6800 [D loss: 0.295911, acc.: 85.94%] [G loss: 2.133558]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "6900 [D loss: 0.420275, acc.: 79.69%] [G loss: 2.171768]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "7000 [D loss: 0.493553, acc.: 74.22%] [G loss: 1.899557]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "7100 [D loss: 0.540341, acc.: 74.22%] [G loss: 2.312440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 62us/step\n",
      "7200 [D loss: 0.368246, acc.: 83.59%] [G loss: 2.185666]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "7300 [D loss: 0.388320, acc.: 83.59%] [G loss: 2.328326]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "7400 [D loss: 0.365134, acc.: 82.03%] [G loss: 2.182276]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "7500 [D loss: 0.388213, acc.: 83.59%] [G loss: 2.378926]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "7600 [D loss: 0.361232, acc.: 82.03%] [G loss: 2.378990]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "7700 [D loss: 0.369328, acc.: 82.81%] [G loss: 2.406241]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "7800 [D loss: 0.434184, acc.: 78.91%] [G loss: 2.198290]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "7900 [D loss: 0.352342, acc.: 81.25%] [G loss: 2.198761]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "8000 [D loss: 0.303526, acc.: 86.72%] [G loss: 2.092170]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "8100 [D loss: 0.468158, acc.: 76.56%] [G loss: 2.013021]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "8200 [D loss: 0.320585, acc.: 85.94%] [G loss: 2.099165]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "8300 [D loss: 0.384878, acc.: 82.81%] [G loss: 1.968371]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "8400 [D loss: 0.452528, acc.: 78.91%] [G loss: 2.064809]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "8500 [D loss: 0.368784, acc.: 85.16%] [G loss: 2.352153]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "8600 [D loss: 0.358477, acc.: 84.38%] [G loss: 2.030329]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "8700 [D loss: 0.361174, acc.: 85.16%] [G loss: 2.409092]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "8800 [D loss: 0.315833, acc.: 88.28%] [G loss: 1.867964]\n",
      "1000/1000 [==============================] - 0s 63us/step\n",
      "8900 [D loss: 0.523434, acc.: 75.78%] [G loss: 1.738447]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "9000 [D loss: 0.356190, acc.: 84.38%] [G loss: 2.195469]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "9100 [D loss: 0.431971, acc.: 76.56%] [G loss: 1.869056]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "9200 [D loss: 0.448938, acc.: 80.47%] [G loss: 2.213547]\n",
      "1000/1000 [==============================] - 0s 63us/step\n",
      "9300 [D loss: 0.511660, acc.: 78.12%] [G loss: 1.926629]\n",
      "1000/1000 [==============================] - 0s 63us/step\n",
      "9400 [D loss: 0.368029, acc.: 82.81%] [G loss: 1.994653]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "9500 [D loss: 0.386128, acc.: 81.25%] [G loss: 2.123389]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "9600 [D loss: 0.484856, acc.: 77.34%] [G loss: 2.005787]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "9700 [D loss: 0.393016, acc.: 83.59%] [G loss: 1.930911]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "9800 [D loss: 0.439627, acc.: 79.69%] [G loss: 1.883633]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "9900 [D loss: 0.343930, acc.: 83.59%] [G loss: 2.166948]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "10000 [D loss: 0.349219, acc.: 85.16%] [G loss: 1.873378]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "10100 [D loss: 0.433422, acc.: 83.59%] [G loss: 2.010471]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "10200 [D loss: 0.363037, acc.: 84.38%] [G loss: 2.270205]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "10300 [D loss: 0.473533, acc.: 75.00%] [G loss: 1.720163]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "10400 [D loss: 0.465984, acc.: 80.47%] [G loss: 2.246018]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "10500 [D loss: 0.501202, acc.: 78.91%] [G loss: 2.006296]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "10600 [D loss: 0.347864, acc.: 87.50%] [G loss: 1.916999]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "10700 [D loss: 0.495095, acc.: 75.00%] [G loss: 1.943455]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "10800 [D loss: 0.513742, acc.: 72.66%] [G loss: 1.805464]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "10900 [D loss: 0.365103, acc.: 85.94%] [G loss: 2.098133]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "11000 [D loss: 0.446605, acc.: 78.12%] [G loss: 2.018354]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "11100 [D loss: 0.417917, acc.: 79.69%] [G loss: 1.916016]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "11200 [D loss: 0.470091, acc.: 78.12%] [G loss: 2.148160]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "11300 [D loss: 0.472422, acc.: 72.66%] [G loss: 1.989527]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "11400 [D loss: 0.469186, acc.: 75.78%] [G loss: 1.929079]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "11500 [D loss: 0.412871, acc.: 83.59%] [G loss: 2.269918]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "11600 [D loss: 0.384426, acc.: 82.81%] [G loss: 2.093106]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "11700 [D loss: 0.424161, acc.: 82.03%] [G loss: 1.800579]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "11800 [D loss: 0.387123, acc.: 80.47%] [G loss: 2.176106]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "11900 [D loss: 0.487080, acc.: 74.22%] [G loss: 1.890814]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "12000 [D loss: 0.521031, acc.: 79.69%] [G loss: 1.933783]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "12100 [D loss: 0.426011, acc.: 82.03%] [G loss: 1.800080]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "12200 [D loss: 0.417796, acc.: 78.91%] [G loss: 1.958771]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "12300 [D loss: 0.487116, acc.: 75.78%] [G loss: 1.655824]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "12400 [D loss: 0.495157, acc.: 75.00%] [G loss: 1.763817]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "12500 [D loss: 0.475669, acc.: 78.91%] [G loss: 1.703020]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "12600 [D loss: 0.395724, acc.: 81.25%] [G loss: 2.002633]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "12700 [D loss: 0.408925, acc.: 79.69%] [G loss: 2.223464]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "12800 [D loss: 0.421651, acc.: 78.91%] [G loss: 2.026453]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "12900 [D loss: 0.458724, acc.: 79.69%] [G loss: 1.958215]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "13000 [D loss: 0.440279, acc.: 78.91%] [G loss: 1.583365]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "13100 [D loss: 0.482249, acc.: 75.78%] [G loss: 1.572937]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "13200 [D loss: 0.428767, acc.: 81.25%] [G loss: 1.970564]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "13300 [D loss: 0.407996, acc.: 82.81%] [G loss: 1.910075]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "13400 [D loss: 0.461822, acc.: 77.34%] [G loss: 2.200116]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "13500 [D loss: 0.452262, acc.: 80.47%] [G loss: 1.915391]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "13600 [D loss: 0.517865, acc.: 73.44%] [G loss: 1.838392]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "13700 [D loss: 0.363575, acc.: 85.16%] [G loss: 1.719867]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "13800 [D loss: 0.432893, acc.: 75.78%] [G loss: 1.889766]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "13900 [D loss: 0.404983, acc.: 80.47%] [G loss: 2.020287]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "14000 [D loss: 0.453026, acc.: 75.78%] [G loss: 1.894665]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "14100 [D loss: 0.469645, acc.: 76.56%] [G loss: 1.882188]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "14200 [D loss: 0.512515, acc.: 77.34%] [G loss: 1.906331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 78us/step\n",
      "14300 [D loss: 0.472158, acc.: 79.69%] [G loss: 1.727091]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "14400 [D loss: 0.504373, acc.: 78.12%] [G loss: 1.564579]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "14500 [D loss: 0.483827, acc.: 78.12%] [G loss: 1.965301]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "14600 [D loss: 0.425013, acc.: 80.47%] [G loss: 1.957249]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "14700 [D loss: 0.446683, acc.: 80.47%] [G loss: 2.026966]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "14800 [D loss: 0.376793, acc.: 87.50%] [G loss: 1.995896]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "14900 [D loss: 0.485926, acc.: 76.56%] [G loss: 2.124198]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "15000 [D loss: 0.439541, acc.: 80.47%] [G loss: 1.786862]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "15100 [D loss: 0.347556, acc.: 85.16%] [G loss: 1.908309]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "15200 [D loss: 0.539293, acc.: 68.75%] [G loss: 1.751801]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "15300 [D loss: 0.433127, acc.: 80.47%] [G loss: 2.060178]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "15400 [D loss: 0.440925, acc.: 77.34%] [G loss: 1.734862]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "15500 [D loss: 0.544529, acc.: 75.00%] [G loss: 1.729585]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "15600 [D loss: 0.534141, acc.: 69.53%] [G loss: 1.853922]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "15700 [D loss: 0.484594, acc.: 74.22%] [G loss: 1.907963]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "15800 [D loss: 0.435615, acc.: 79.69%] [G loss: 1.777411]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "15900 [D loss: 0.403309, acc.: 82.03%] [G loss: 1.804866]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "16000 [D loss: 0.429628, acc.: 82.03%] [G loss: 1.793573]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "16100 [D loss: 0.397756, acc.: 80.47%] [G loss: 1.755932]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "16200 [D loss: 0.460285, acc.: 78.12%] [G loss: 2.063718]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "16300 [D loss: 0.422664, acc.: 79.69%] [G loss: 1.872976]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "16400 [D loss: 0.491116, acc.: 75.00%] [G loss: 1.614790]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "16500 [D loss: 0.438405, acc.: 78.91%] [G loss: 1.697206]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "16600 [D loss: 0.456686, acc.: 80.47%] [G loss: 1.701088]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "16700 [D loss: 0.407474, acc.: 82.81%] [G loss: 1.680823]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "16800 [D loss: 0.427955, acc.: 78.91%] [G loss: 1.930451]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "16900 [D loss: 0.424829, acc.: 82.81%] [G loss: 1.888585]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "17000 [D loss: 0.528536, acc.: 78.12%] [G loss: 1.732365]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "17100 [D loss: 0.428963, acc.: 78.91%] [G loss: 1.742353]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "17200 [D loss: 0.421552, acc.: 80.47%] [G loss: 1.712693]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "17300 [D loss: 0.438678, acc.: 75.00%] [G loss: 1.713348]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "17400 [D loss: 0.366409, acc.: 85.16%] [G loss: 1.862379]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "17500 [D loss: 0.491243, acc.: 75.78%] [G loss: 1.681632]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "17600 [D loss: 0.420250, acc.: 79.69%] [G loss: 1.811451]\n",
      "1000/1000 [==============================] - 0s 63us/step\n",
      "17700 [D loss: 0.506172, acc.: 75.00%] [G loss: 1.817140]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "17800 [D loss: 0.532711, acc.: 74.22%] [G loss: 1.808600]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "17900 [D loss: 0.432064, acc.: 79.69%] [G loss: 1.685025]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "18000 [D loss: 0.492174, acc.: 75.78%] [G loss: 1.894694]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "18100 [D loss: 0.436905, acc.: 80.47%] [G loss: 1.735131]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "18200 [D loss: 0.423941, acc.: 82.81%] [G loss: 1.705015]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "18300 [D loss: 0.516205, acc.: 75.00%] [G loss: 1.851043]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "18400 [D loss: 0.419443, acc.: 80.47%] [G loss: 1.551818]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "18500 [D loss: 0.439913, acc.: 79.69%] [G loss: 1.762235]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "18600 [D loss: 0.495175, acc.: 76.56%] [G loss: 1.770370]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "18700 [D loss: 0.410220, acc.: 81.25%] [G loss: 1.636293]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "18800 [D loss: 0.413349, acc.: 82.03%] [G loss: 1.743513]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "18900 [D loss: 0.469595, acc.: 75.00%] [G loss: 1.813788]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "19000 [D loss: 0.465040, acc.: 75.78%] [G loss: 1.646625]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "19100 [D loss: 0.430960, acc.: 75.78%] [G loss: 1.953152]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "19200 [D loss: 0.466160, acc.: 71.88%] [G loss: 1.917441]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "19300 [D loss: 0.525055, acc.: 72.66%] [G loss: 1.629141]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "19400 [D loss: 0.459891, acc.: 78.12%] [G loss: 1.801748]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "19500 [D loss: 0.466149, acc.: 78.12%] [G loss: 1.688247]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "19600 [D loss: 0.456903, acc.: 82.81%] [G loss: 1.675619]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "19700 [D loss: 0.401946, acc.: 79.69%] [G loss: 1.586701]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "19800 [D loss: 0.399647, acc.: 84.38%] [G loss: 1.772104]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "19900 [D loss: 0.377569, acc.: 78.91%] [G loss: 1.719390]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "20000 [D loss: 0.427848, acc.: 82.03%] [G loss: 1.771849]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "20100 [D loss: 0.489297, acc.: 75.00%] [G loss: 1.634874]\n",
      "1000/1000 [==============================] - 0s 109us/step\n",
      "20200 [D loss: 0.479887, acc.: 71.88%] [G loss: 1.576655]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "20300 [D loss: 0.486899, acc.: 73.44%] [G loss: 1.520474]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "20400 [D loss: 0.436816, acc.: 77.34%] [G loss: 1.522475]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "20500 [D loss: 0.483821, acc.: 75.00%] [G loss: 1.440382]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "20600 [D loss: 0.498348, acc.: 71.88%] [G loss: 1.644945]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "20700 [D loss: 0.529299, acc.: 70.31%] [G loss: 1.670939]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "20800 [D loss: 0.482958, acc.: 76.56%] [G loss: 1.747095]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "20900 [D loss: 0.475159, acc.: 76.56%] [G loss: 1.764954]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "21000 [D loss: 0.521301, acc.: 74.22%] [G loss: 1.548924]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "21100 [D loss: 0.455800, acc.: 77.34%] [G loss: 1.630972]\n",
      "1000/1000 [==============================] - 0s 109us/step\n",
      "21200 [D loss: 0.499367, acc.: 77.34%] [G loss: 1.949255]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "21300 [D loss: 0.473743, acc.: 76.56%] [G loss: 1.689864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 78us/step\n",
      "21400 [D loss: 0.533486, acc.: 75.78%] [G loss: 1.595215]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "21500 [D loss: 0.497422, acc.: 75.00%] [G loss: 1.774763]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "21600 [D loss: 0.507160, acc.: 71.88%] [G loss: 1.564943]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "21700 [D loss: 0.461737, acc.: 78.91%] [G loss: 1.569417]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "21800 [D loss: 0.423375, acc.: 80.47%] [G loss: 1.632751]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "21900 [D loss: 0.499811, acc.: 75.78%] [G loss: 1.867285]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "22000 [D loss: 0.515944, acc.: 75.00%] [G loss: 1.622117]\n",
      "1000/1000 [==============================] - 0s 109us/step\n",
      "22100 [D loss: 0.497440, acc.: 81.25%] [G loss: 1.751449]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "22200 [D loss: 0.494884, acc.: 78.91%] [G loss: 1.708250]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "22300 [D loss: 0.434319, acc.: 78.12%] [G loss: 1.722980]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "22400 [D loss: 0.440075, acc.: 81.25%] [G loss: 1.880029]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "22500 [D loss: 0.546231, acc.: 69.53%] [G loss: 1.610985]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "22600 [D loss: 0.457967, acc.: 75.78%] [G loss: 1.613099]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "22700 [D loss: 0.467045, acc.: 78.91%] [G loss: 1.838991]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "22800 [D loss: 0.501802, acc.: 76.56%] [G loss: 1.809767]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "22900 [D loss: 0.522872, acc.: 73.44%] [G loss: 1.657466]\n",
      "1000/1000 [==============================] - 0s 125us/step\n",
      "23000 [D loss: 0.450784, acc.: 78.91%] [G loss: 1.510305]\n",
      "1000/1000 [==============================] - 0s 63us/step\n",
      "23100 [D loss: 0.433424, acc.: 78.12%] [G loss: 1.794947]\n",
      "1000/1000 [==============================] - 0s 109us/step\n",
      "23200 [D loss: 0.562575, acc.: 73.44%] [G loss: 1.600294]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "23300 [D loss: 0.464119, acc.: 80.47%] [G loss: 1.585572]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "23400 [D loss: 0.519361, acc.: 73.44%] [G loss: 1.705668]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "23500 [D loss: 0.432956, acc.: 77.34%] [G loss: 1.613791]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "23600 [D loss: 0.492084, acc.: 74.22%] [G loss: 1.832943]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "23700 [D loss: 0.502190, acc.: 73.44%] [G loss: 1.533517]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "23800 [D loss: 0.479593, acc.: 77.34%] [G loss: 1.662256]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "23900 [D loss: 0.488901, acc.: 75.78%] [G loss: 1.603120]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "24000 [D loss: 0.413030, acc.: 80.47%] [G loss: 1.713401]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "24100 [D loss: 0.473130, acc.: 76.56%] [G loss: 1.449222]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "24200 [D loss: 0.443436, acc.: 78.91%] [G loss: 2.043210]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "24300 [D loss: 0.449979, acc.: 75.78%] [G loss: 1.657824]\n",
      "1000/1000 [==============================] - 0s 63us/step\n",
      "24400 [D loss: 0.480366, acc.: 76.56%] [G loss: 1.699048]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "24500 [D loss: 0.452943, acc.: 78.91%] [G loss: 1.620096]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "24600 [D loss: 0.461296, acc.: 77.34%] [G loss: 1.437411]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "24700 [D loss: 0.460361, acc.: 79.69%] [G loss: 1.570094]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "24800 [D loss: 0.431995, acc.: 87.50%] [G loss: 1.791160]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "24900 [D loss: 0.432075, acc.: 78.91%] [G loss: 1.717745]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "25000 [D loss: 0.401571, acc.: 81.25%] [G loss: 1.686022]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "25100 [D loss: 0.465572, acc.: 76.56%] [G loss: 1.524546]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "25200 [D loss: 0.463442, acc.: 76.56%] [G loss: 1.687031]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "25300 [D loss: 0.436854, acc.: 78.12%] [G loss: 1.630847]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "25400 [D loss: 0.496957, acc.: 70.31%] [G loss: 1.720593]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "25500 [D loss: 0.473763, acc.: 74.22%] [G loss: 1.653083]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "25600 [D loss: 0.447075, acc.: 85.16%] [G loss: 1.593452]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "25700 [D loss: 0.456050, acc.: 78.91%] [G loss: 1.948514]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "25800 [D loss: 0.561154, acc.: 72.66%] [G loss: 1.720489]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "25900 [D loss: 0.432285, acc.: 78.91%] [G loss: 1.621315]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "26000 [D loss: 0.481604, acc.: 75.78%] [G loss: 1.424527]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "26100 [D loss: 0.468809, acc.: 76.56%] [G loss: 1.772619]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "26200 [D loss: 0.506445, acc.: 74.22%] [G loss: 1.551662]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "26300 [D loss: 0.450229, acc.: 77.34%] [G loss: 1.727779]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "26400 [D loss: 0.418538, acc.: 76.56%] [G loss: 1.816772]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "26500 [D loss: 0.482960, acc.: 73.44%] [G loss: 1.645147]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "26600 [D loss: 0.557047, acc.: 71.88%] [G loss: 1.682896]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "26700 [D loss: 0.542758, acc.: 72.66%] [G loss: 1.942325]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "26800 [D loss: 0.333802, acc.: 85.16%] [G loss: 1.718373]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "26900 [D loss: 0.471848, acc.: 75.78%] [G loss: 1.726598]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "27000 [D loss: 0.363050, acc.: 85.16%] [G loss: 1.522669]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "27100 [D loss: 0.514589, acc.: 76.56%] [G loss: 1.683087]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "27200 [D loss: 0.464674, acc.: 73.44%] [G loss: 1.736263]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "27300 [D loss: 0.431422, acc.: 78.91%] [G loss: 1.526268]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "27400 [D loss: 0.425071, acc.: 78.91%] [G loss: 1.554139]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "27500 [D loss: 0.425957, acc.: 79.69%] [G loss: 1.670278]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "27600 [D loss: 0.519980, acc.: 71.09%] [G loss: 1.577610]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "27700 [D loss: 0.518583, acc.: 75.00%] [G loss: 1.848716]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "27800 [D loss: 0.409847, acc.: 78.12%] [G loss: 2.012984]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "27900 [D loss: 0.509848, acc.: 71.09%] [G loss: 1.594995]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "28000 [D loss: 0.473705, acc.: 75.78%] [G loss: 1.562772]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "28100 [D loss: 0.468911, acc.: 78.12%] [G loss: 1.556911]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "28200 [D loss: 0.540578, acc.: 72.66%] [G loss: 1.527066]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "28300 [D loss: 0.404344, acc.: 82.81%] [G loss: 1.656745]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "28400 [D loss: 0.458521, acc.: 80.47%] [G loss: 1.775352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 78us/step\n",
      "28500 [D loss: 0.543051, acc.: 70.31%] [G loss: 1.750896]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "28600 [D loss: 0.391862, acc.: 83.59%] [G loss: 1.572588]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "28700 [D loss: 0.371014, acc.: 83.59%] [G loss: 1.753046]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "28800 [D loss: 0.414468, acc.: 83.59%] [G loss: 1.579420]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "28900 [D loss: 0.435506, acc.: 78.12%] [G loss: 1.702550]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "29000 [D loss: 0.420761, acc.: 78.91%] [G loss: 1.639980]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "29100 [D loss: 0.493300, acc.: 75.00%] [G loss: 1.839269]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "29200 [D loss: 0.582810, acc.: 71.88%] [G loss: 1.842975]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "29300 [D loss: 0.458464, acc.: 78.12%] [G loss: 1.940878]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "29400 [D loss: 0.469159, acc.: 77.34%] [G loss: 1.695428]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "29500 [D loss: 0.472108, acc.: 73.44%] [G loss: 1.775139]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "29600 [D loss: 0.451006, acc.: 79.69%] [G loss: 1.593442]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "29700 [D loss: 0.398767, acc.: 81.25%] [G loss: 1.618565]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "29800 [D loss: 0.461570, acc.: 81.25%] [G loss: 1.771581]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "29900 [D loss: 0.431727, acc.: 78.12%] [G loss: 1.718221]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "30000 [D loss: 0.409589, acc.: 80.47%] [G loss: 1.511424]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "30100 [D loss: 0.460184, acc.: 75.78%] [G loss: 1.964414]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "30200 [D loss: 0.488679, acc.: 75.78%] [G loss: 1.471998]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "30300 [D loss: 0.489557, acc.: 70.31%] [G loss: 1.757100]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "30400 [D loss: 0.610444, acc.: 67.97%] [G loss: 1.785902]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "30500 [D loss: 0.474664, acc.: 75.78%] [G loss: 1.698235]\n",
      "1000/1000 [==============================] - 0s 63us/step\n",
      "30600 [D loss: 0.538593, acc.: 67.19%] [G loss: 1.985893]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "30700 [D loss: 0.555950, acc.: 70.31%] [G loss: 1.553869]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "30800 [D loss: 0.480461, acc.: 75.78%] [G loss: 1.557297]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "30900 [D loss: 0.488435, acc.: 74.22%] [G loss: 1.780237]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "31000 [D loss: 0.436457, acc.: 79.69%] [G loss: 1.615099]\n",
      "1000/1000 [==============================] - 0s 63us/step\n",
      "31100 [D loss: 0.487583, acc.: 74.22%] [G loss: 1.473732]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "31200 [D loss: 0.461142, acc.: 83.59%] [G loss: 1.703900]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "31300 [D loss: 0.555707, acc.: 70.31%] [G loss: 1.662551]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "31400 [D loss: 0.486953, acc.: 75.00%] [G loss: 1.649094]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "31500 [D loss: 0.537844, acc.: 71.09%] [G loss: 1.385249]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "31600 [D loss: 0.493943, acc.: 78.12%] [G loss: 1.659681]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "31700 [D loss: 0.451121, acc.: 77.34%] [G loss: 1.635594]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "31800 [D loss: 0.474553, acc.: 77.34%] [G loss: 1.617945]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "31900 [D loss: 0.496813, acc.: 76.56%] [G loss: 1.727309]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "32000 [D loss: 0.476619, acc.: 72.66%] [G loss: 1.884312]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "32100 [D loss: 0.458543, acc.: 77.34%] [G loss: 1.627713]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "32200 [D loss: 0.522436, acc.: 72.66%] [G loss: 1.845588]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "32300 [D loss: 0.466753, acc.: 78.91%] [G loss: 1.532996]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "32400 [D loss: 0.483894, acc.: 77.34%] [G loss: 1.659271]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "32500 [D loss: 0.517621, acc.: 72.66%] [G loss: 1.613486]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "32600 [D loss: 0.500434, acc.: 76.56%] [G loss: 1.642599]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "32700 [D loss: 0.447671, acc.: 78.12%] [G loss: 1.672601]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "32800 [D loss: 0.505967, acc.: 75.00%] [G loss: 1.371733]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "32900 [D loss: 0.417292, acc.: 80.47%] [G loss: 1.678762]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "33000 [D loss: 0.426102, acc.: 82.03%] [G loss: 1.503177]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "33100 [D loss: 0.529182, acc.: 74.22%] [G loss: 1.699027]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "33200 [D loss: 0.437590, acc.: 79.69%] [G loss: 1.853223]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "33300 [D loss: 0.456991, acc.: 78.12%] [G loss: 1.720444]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "33400 [D loss: 0.441504, acc.: 77.34%] [G loss: 1.695370]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "33500 [D loss: 0.530094, acc.: 66.41%] [G loss: 1.585013]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "33600 [D loss: 0.438070, acc.: 78.12%] [G loss: 1.665232]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "33700 [D loss: 0.489444, acc.: 76.56%] [G loss: 1.586522]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "33800 [D loss: 0.447748, acc.: 75.78%] [G loss: 1.856096]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "33900 [D loss: 0.508768, acc.: 72.66%] [G loss: 1.516805]\n",
      "1000/1000 [==============================] - 0s 125us/step\n",
      "34000 [D loss: 0.508668, acc.: 71.09%] [G loss: 1.941245]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "34100 [D loss: 0.465633, acc.: 77.34%] [G loss: 1.742801]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "34200 [D loss: 0.562661, acc.: 71.09%] [G loss: 1.935039]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "34300 [D loss: 0.447909, acc.: 79.69%] [G loss: 1.578061]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "34400 [D loss: 0.480915, acc.: 77.34%] [G loss: 1.605182]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "34500 [D loss: 0.527631, acc.: 74.22%] [G loss: 1.544001]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "34600 [D loss: 0.422173, acc.: 80.47%] [G loss: 1.607357]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "34700 [D loss: 0.465240, acc.: 78.91%] [G loss: 1.641965]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "34800 [D loss: 0.471526, acc.: 79.69%] [G loss: 1.656590]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "34900 [D loss: 0.379925, acc.: 84.38%] [G loss: 1.447100]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "35000 [D loss: 0.503225, acc.: 72.66%] [G loss: 1.607008]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "35100 [D loss: 0.439551, acc.: 82.03%] [G loss: 1.580381]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "35200 [D loss: 0.556622, acc.: 69.53%] [G loss: 1.652879]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "35300 [D loss: 0.452310, acc.: 76.56%] [G loss: 1.574817]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "35400 [D loss: 0.507609, acc.: 75.00%] [G loss: 1.868022]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "35500 [D loss: 0.398647, acc.: 80.47%] [G loss: 1.666031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 78us/step\n",
      "35600 [D loss: 0.506682, acc.: 74.22%] [G loss: 1.824470]\n",
      "1000/1000 [==============================] - 0s 109us/step\n",
      "35700 [D loss: 0.512546, acc.: 71.88%] [G loss: 1.556720]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "35800 [D loss: 0.406875, acc.: 77.34%] [G loss: 1.879653]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "35900 [D loss: 0.476935, acc.: 78.91%] [G loss: 1.799596]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "36000 [D loss: 0.467124, acc.: 79.69%] [G loss: 1.612068]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "36100 [D loss: 0.491347, acc.: 75.00%] [G loss: 1.702891]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "36200 [D loss: 0.503136, acc.: 74.22%] [G loss: 1.902824]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "36300 [D loss: 0.539725, acc.: 71.88%] [G loss: 1.908020]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "36400 [D loss: 0.466357, acc.: 81.25%] [G loss: 1.685573]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "36500 [D loss: 0.602391, acc.: 66.41%] [G loss: 1.516291]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "36600 [D loss: 0.369572, acc.: 81.25%] [G loss: 1.805310]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "36700 [D loss: 0.554653, acc.: 67.19%] [G loss: 1.654241]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "36800 [D loss: 0.545825, acc.: 71.88%] [G loss: 1.559699]\n",
      "1000/1000 [==============================] - 0s 125us/step\n",
      "36900 [D loss: 0.421971, acc.: 81.25%] [G loss: 1.692035]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "37000 [D loss: 0.457185, acc.: 79.69%] [G loss: 1.271957]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "37100 [D loss: 0.545514, acc.: 73.44%] [G loss: 1.593531]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "37200 [D loss: 0.415946, acc.: 81.25%] [G loss: 1.529755]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "37300 [D loss: 0.496340, acc.: 73.44%] [G loss: 1.403610]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "37400 [D loss: 0.417054, acc.: 80.47%] [G loss: 1.681419]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "37500 [D loss: 0.503643, acc.: 75.78%] [G loss: 1.541958]\n",
      "1000/1000 [==============================] - 0s 63us/step\n",
      "37600 [D loss: 0.492544, acc.: 75.78%] [G loss: 1.795406]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "37700 [D loss: 0.593884, acc.: 66.41%] [G loss: 1.446155]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "37800 [D loss: 0.492241, acc.: 75.78%] [G loss: 1.517637]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "37900 [D loss: 0.508353, acc.: 71.09%] [G loss: 1.681996]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "38000 [D loss: 0.457229, acc.: 78.12%] [G loss: 1.453706]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "38100 [D loss: 0.548208, acc.: 72.66%] [G loss: 1.551943]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "38200 [D loss: 0.510737, acc.: 75.00%] [G loss: 1.501153]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "38300 [D loss: 0.437083, acc.: 79.69%] [G loss: 1.868846]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "38400 [D loss: 0.460122, acc.: 78.12%] [G loss: 1.906145]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "38500 [D loss: 0.524294, acc.: 66.41%] [G loss: 1.673741]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "38600 [D loss: 0.553146, acc.: 68.75%] [G loss: 1.552444]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "38700 [D loss: 0.438080, acc.: 78.12%] [G loss: 1.724047]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "38800 [D loss: 0.472299, acc.: 81.25%] [G loss: 1.644268]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "38900 [D loss: 0.465041, acc.: 78.12%] [G loss: 1.641603]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "39000 [D loss: 0.469202, acc.: 75.00%] [G loss: 1.577460]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "39100 [D loss: 0.475085, acc.: 76.56%] [G loss: 1.824812]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "39200 [D loss: 0.521704, acc.: 68.75%] [G loss: 1.664601]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "39300 [D loss: 0.460625, acc.: 81.25%] [G loss: 1.843618]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "39400 [D loss: 0.476492, acc.: 75.00%] [G loss: 1.827682]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "39500 [D loss: 0.450222, acc.: 78.12%] [G loss: 1.819953]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "39600 [D loss: 0.480626, acc.: 71.09%] [G loss: 1.754234]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "39700 [D loss: 0.482208, acc.: 73.44%] [G loss: 1.519218]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "39800 [D loss: 0.535104, acc.: 74.22%] [G loss: 1.654604]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "39900 [D loss: 0.465957, acc.: 78.91%] [G loss: 1.642937]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "40000 [D loss: 0.507092, acc.: 76.56%] [G loss: 1.697470]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "40100 [D loss: 0.470292, acc.: 71.88%] [G loss: 1.718263]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "40200 [D loss: 0.507371, acc.: 72.66%] [G loss: 1.667491]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "40300 [D loss: 0.387173, acc.: 83.59%] [G loss: 1.734407]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "40400 [D loss: 0.493374, acc.: 76.56%] [G loss: 1.860504]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "40500 [D loss: 0.397282, acc.: 81.25%] [G loss: 1.728424]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "40600 [D loss: 0.520096, acc.: 73.44%] [G loss: 1.678922]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "40700 [D loss: 0.493379, acc.: 71.88%] [G loss: 1.502835]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "40800 [D loss: 0.431280, acc.: 81.25%] [G loss: 1.680637]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "40900 [D loss: 0.474366, acc.: 77.34%] [G loss: 1.724886]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "41000 [D loss: 0.456668, acc.: 77.34%] [G loss: 1.759735]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "41100 [D loss: 0.604836, acc.: 66.41%] [G loss: 1.694281]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "41200 [D loss: 0.466778, acc.: 72.66%] [G loss: 1.647547]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "41300 [D loss: 0.496369, acc.: 78.12%] [G loss: 1.777284]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "41400 [D loss: 0.510958, acc.: 74.22%] [G loss: 1.584847]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "41500 [D loss: 0.497710, acc.: 76.56%] [G loss: 1.661031]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "41600 [D loss: 0.514860, acc.: 73.44%] [G loss: 1.507141]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "41700 [D loss: 0.624157, acc.: 64.06%] [G loss: 1.665150]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "41800 [D loss: 0.483164, acc.: 78.91%] [G loss: 1.708061]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "41900 [D loss: 0.471200, acc.: 78.12%] [G loss: 1.798986]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "42000 [D loss: 0.498605, acc.: 72.66%] [G loss: 1.581045]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "42100 [D loss: 0.479511, acc.: 71.88%] [G loss: 1.370718]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "42200 [D loss: 0.544926, acc.: 69.53%] [G loss: 1.557089]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "42300 [D loss: 0.529994, acc.: 73.44%] [G loss: 1.643999]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "42400 [D loss: 0.535686, acc.: 73.44%] [G loss: 1.544662]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "42500 [D loss: 0.529681, acc.: 71.88%] [G loss: 1.528263]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "42600 [D loss: 0.530947, acc.: 70.31%] [G loss: 1.540206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 78us/step\n",
      "42700 [D loss: 0.486289, acc.: 78.12%] [G loss: 1.484427]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "42800 [D loss: 0.489846, acc.: 70.31%] [G loss: 1.445642]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "42900 [D loss: 0.496200, acc.: 76.56%] [G loss: 1.679329]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "43000 [D loss: 0.494158, acc.: 72.66%] [G loss: 1.592707]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "43100 [D loss: 0.389699, acc.: 82.03%] [G loss: 1.597959]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "43200 [D loss: 0.443284, acc.: 76.56%] [G loss: 1.426397]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "43300 [D loss: 0.517381, acc.: 71.88%] [G loss: 1.566092]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "43400 [D loss: 0.601220, acc.: 70.31%] [G loss: 1.696341]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "43500 [D loss: 0.528337, acc.: 71.09%] [G loss: 1.645567]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "43600 [D loss: 0.557822, acc.: 73.44%] [G loss: 1.344946]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "43700 [D loss: 0.548432, acc.: 69.53%] [G loss: 1.212791]\n",
      "1000/1000 [==============================] - 0s 109us/step\n",
      "43800 [D loss: 0.534747, acc.: 74.22%] [G loss: 1.646927]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "43900 [D loss: 0.433294, acc.: 79.69%] [G loss: 1.825699]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "44000 [D loss: 0.502599, acc.: 75.00%] [G loss: 1.360114]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "44100 [D loss: 0.438924, acc.: 77.34%] [G loss: 1.496757]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "44200 [D loss: 0.493830, acc.: 73.44%] [G loss: 1.692491]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "44300 [D loss: 0.565077, acc.: 67.97%] [G loss: 1.783659]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "44400 [D loss: 0.428260, acc.: 80.47%] [G loss: 1.655576]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "44500 [D loss: 0.453956, acc.: 82.03%] [G loss: 1.633938]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "44600 [D loss: 0.569424, acc.: 68.75%] [G loss: 1.531314]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "44700 [D loss: 0.438459, acc.: 77.34%] [G loss: 1.644406]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "44800 [D loss: 0.455728, acc.: 75.78%] [G loss: 1.864820]\n",
      "1000/1000 [==============================] - 0s 63us/step\n",
      "44900 [D loss: 0.564866, acc.: 70.31%] [G loss: 1.827996]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "45000 [D loss: 0.526593, acc.: 72.66%] [G loss: 1.642900]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "45100 [D loss: 0.548951, acc.: 72.66%] [G loss: 1.810512]\n",
      "1000/1000 [==============================] - 0s 141us/step\n",
      "45200 [D loss: 0.503640, acc.: 75.78%] [G loss: 1.479259]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "45300 [D loss: 0.449645, acc.: 75.00%] [G loss: 1.875599]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "45400 [D loss: 0.545043, acc.: 73.44%] [G loss: 1.500252]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "45500 [D loss: 0.474838, acc.: 75.78%] [G loss: 1.440145]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "45600 [D loss: 0.453488, acc.: 77.34%] [G loss: 1.703397]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "45700 [D loss: 0.547056, acc.: 73.44%] [G loss: 1.444340]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "45800 [D loss: 0.487102, acc.: 75.78%] [G loss: 1.810594]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "45900 [D loss: 0.423817, acc.: 77.34%] [G loss: 1.742993]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "46000 [D loss: 0.474294, acc.: 75.78%] [G loss: 1.641807]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "46100 [D loss: 0.499984, acc.: 74.22%] [G loss: 1.856250]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "46200 [D loss: 0.510973, acc.: 74.22%] [G loss: 1.704191]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "46300 [D loss: 0.570105, acc.: 72.66%] [G loss: 1.678338]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "46400 [D loss: 0.519383, acc.: 72.66%] [G loss: 1.508732]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "46500 [D loss: 0.565733, acc.: 69.53%] [G loss: 1.591121]\n",
      "1000/1000 [==============================] - 0s 109us/step\n",
      "46600 [D loss: 0.382364, acc.: 82.81%] [G loss: 1.610454]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "46700 [D loss: 0.458552, acc.: 78.12%] [G loss: 1.600043]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "46800 [D loss: 0.406830, acc.: 82.81%] [G loss: 1.687444]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "46900 [D loss: 0.370623, acc.: 87.50%] [G loss: 1.516668]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "47000 [D loss: 0.466857, acc.: 75.78%] [G loss: 1.656442]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "47100 [D loss: 0.416416, acc.: 77.34%] [G loss: 1.805359]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "47200 [D loss: 0.493236, acc.: 74.22%] [G loss: 1.805600]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "47300 [D loss: 0.503320, acc.: 77.34%] [G loss: 1.483258]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "47400 [D loss: 0.591310, acc.: 69.53%] [G loss: 1.519103]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "47500 [D loss: 0.488761, acc.: 72.66%] [G loss: 1.703305]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "47600 [D loss: 0.509716, acc.: 75.78%] [G loss: 1.573519]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "47700 [D loss: 0.489635, acc.: 76.56%] [G loss: 1.722653]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "47800 [D loss: 0.496545, acc.: 75.78%] [G loss: 1.752663]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "47900 [D loss: 0.432460, acc.: 75.78%] [G loss: 1.642374]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "48000 [D loss: 0.457355, acc.: 80.47%] [G loss: 1.448119]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "48100 [D loss: 0.531809, acc.: 75.78%] [G loss: 1.846718]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "48200 [D loss: 0.476372, acc.: 71.88%] [G loss: 1.571581]\n",
      "1000/1000 [==============================] - 0s 125us/step\n",
      "48300 [D loss: 0.518578, acc.: 72.66%] [G loss: 1.701035]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "48400 [D loss: 0.548079, acc.: 74.22%] [G loss: 1.681634]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "48500 [D loss: 0.474887, acc.: 75.00%] [G loss: 1.452554]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "48600 [D loss: 0.511491, acc.: 71.88%] [G loss: 1.534837]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "48700 [D loss: 0.540305, acc.: 70.31%] [G loss: 1.550572]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "48800 [D loss: 0.487978, acc.: 72.66%] [G loss: 1.387993]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "48900 [D loss: 0.548015, acc.: 73.44%] [G loss: 1.819945]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "49000 [D loss: 0.544736, acc.: 68.75%] [G loss: 1.623004]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "49100 [D loss: 0.534688, acc.: 70.31%] [G loss: 1.395401]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "49200 [D loss: 0.540683, acc.: 72.66%] [G loss: 1.664742]\n",
      "1000/1000 [==============================] - 0s 94us/step\n",
      "49300 [D loss: 0.466861, acc.: 75.78%] [G loss: 1.756627]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "49400 [D loss: 0.498154, acc.: 74.22%] [G loss: 1.610366]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "49500 [D loss: 0.444410, acc.: 77.34%] [G loss: 1.728875]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "49600 [D loss: 0.466733, acc.: 78.12%] [G loss: 1.837622]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "49700 [D loss: 0.508859, acc.: 77.34%] [G loss: 1.579112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 78us/step\n",
      "49800 [D loss: 0.484242, acc.: 78.12%] [G loss: 1.837407]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "49900 [D loss: 0.521507, acc.: 74.22%] [G loss: 1.676133]\n",
      "1000/1000 [==============================] - 0s 78us/step\n",
      "50000 [D loss: 0.531329, acc.: 71.88%] [G loss: 1.530440]\n",
      "1000/1000 [==============================] - 0s 62us/step\n",
      "elapsed training time: 19 min, 2 sec \n",
      "10000/10000 [==============================] - 1s 70us/step\n",
      "10000/10000 [==============================] - 1s 77us/step\n"
     ]
    }
   ],
   "source": [
    "ratio_array = [ (1,5) ]\n",
    "# ratio_array = [ (1,1),(2,1), (1,2), (3,1), (1,3), (5,1), (1,5), (1,10), (10,1)]\n",
    "\n",
    "# model_array = ['Shallow_simple', 'Shallow_BN'  ]\n",
    "model_array = ['Deep_BN' ]\n",
    "\n",
    "# model_array = ['Deep_BN' 'Deeper_G_BN']\n",
    "\n",
    "\n",
    "for pick_model in model_array:\n",
    "    if pick_model != model_array[0]:\n",
    "        del cgan\n",
    "    #---------------------------COMPILE SELECTED MODELS--------------------------------------#\n",
    "    #----------------------------------------LOOP OVER RATIOS--------------------------------#\n",
    "    for ratio in ratio_array:\n",
    "        if ratio != ratio_array[0]:\n",
    "            del cgan\n",
    "            \n",
    "        cgan = CGAN(pick_model=pick_model)\n",
    "\n",
    "         #----------------------------------------EXECUTION-----------------------------------#\n",
    "        start = time.time()\n",
    "\n",
    "        cgan.train(epochs=50001, batch_size=64, sample_interval=100, ratio=ratio, pick_model=pick_model) ## ratio G:D\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        elapsed_train_time = 'elapsed training time: {} min, {} sec '.format(int((end - start) / 60),\n",
    "                                                                             int((end - start) % 60))\n",
    "        train_hist['total_ptime'].append(elapsed_train_time)\n",
    "\n",
    "        print(elapsed_train_time)\n",
    "        os.makedirs(root + 'hist/', exist_ok=True)  \n",
    "        show_train_hist(train_hist, save=True, path=root + 'hist/' + str(ratio[0]) + '_' + str(ratio[1]) + pick_model +'.png')\n",
    "        # save hist data to csv\n",
    "        gen_images, gen_labels = cgan.generate_images(1000)\n",
    "        loss, acc = classif.evaluate(gen_images, to_categorical(gen_labels))\n",
    "        train_hist['classif_loss'].append(loss)\n",
    "        train_hist['classif_acc'].append(acc)\n",
    "        \n",
    "        os.makedirs(root + 'hist_csv/', exist_ok=True) \n",
    "        with open(root+ 'hist_csv/' + str(ratio[0]) + '_' + str(ratio[1]) + pick_model +'.csv', 'w') as f:\n",
    "            for key in train_hist.keys():\n",
    "                f.write(\"%s,%s\\n\"%(key,train_hist[key]))\n",
    "        \n",
    "        \n",
    "        # remove old data structure\n",
    "        del train_hist\n",
    "        # redefine\n",
    "        train_hist = {}\n",
    "        train_hist['D_losses'] = []\n",
    "        train_hist['G_losses'] = []\n",
    "        train_hist['per_epoch_ptimes'] = []\n",
    "        train_hist['total_ptime'] = []\n",
    "        train_hist['accuracy'] = []\n",
    "        train_hist['Model'] = []\n",
    "        train_hist['classif_loss'] = []\n",
    "        train_hist['classif_acc'] = []\n",
    "        gen_images, gen_labels = cgan.generate_images(1000)\n",
    "\n",
    "        loss, acc = classif.evaluate(gen_images, to_categorical(gen_labels))\n",
    "        \n",
    "#         os.makedirs('saved_model_weights/generator_weights/', exist_ok=True)\n",
    "#         os.makedirs('saved_model_weights/discriminator_weights/', exist_ok=True)\n",
    "#         os.makedirs('saved_model_weights/combined_weights/', exist_ok=True)\n",
    "#         generator.save_weights('saved_model_weights/generator_weights/' + pick_model + '_' + ratio + '_' + epoch + '.h5')\n",
    "#         discriminator.save_weights('saved_model_weights/discriminator_weights/' + pick_model + '_' + ratio + '_' + epoch + '.h5')\n",
    "#         combined.save_weights('saved_model_weights/combined_weights-%d.h5' + pick_model + '_' + ratio + '_' + epoch + '.h5')\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gen_images.shape)\n",
    "# image_out = gen_images.reshape(10000,784)\n",
    "# print(image_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(image_out)\n",
    "# df.to_csv('generated_images.csv',index=False, header=False)\n",
    "# dfl = pd.DataFrame(gen_labels)\n",
    "# dfl.to_csv('generated_labels.csv',index=False, header=False)\n",
    "# epoch = 0\n",
    "# acc = 0.5\n",
    "# gen_images, gen_labels = cgan.generate_images(10000)\n",
    "# image_out = gen_images.reshape(100000,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(image_out)\n",
    "# df.to_csv('generated_images_%d'%epoch +'_%f.csv'%acc, index=False, header=False)\n",
    "# dfl = pd.DataFrame(gen_labels)\n",
    "# dfl.to_csv('generated_labels_%d'%epoch +'_%f.csv'%acc, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_images_raw, gen_labels = cgan.generate_images(1000)\n",
    "# # gen_imgs = 0.5 * gen_images + 0.5\n",
    "# gen_images = ((gen_images_raw.astype(np.float32) * 255)) \n",
    "\n",
    "# gen_labels_cat = to_categorical(gen_labels)\n",
    "# loss, acc = classif.evaluate(gen_images, gen_labels_cat)\n",
    "# print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gen_images[0])\n",
    "# print(X_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gen_labels_cat[9])\n",
    "# print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classif.evaluate(X_test[0:5], y_test[0:5]))\n",
    "# print(classif.predict(X_test[0:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(loss)\n",
    "# print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_test.shape)\n",
    "# print(y_test.shape)\n",
    "# print(gen_images.shape)\n",
    "# print(to_categorical(gen_labels).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(to_categorical(gen_labels))\n",
    "# print(classif.predict(X_test))\n",
    "# to_categorical(gen_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gen_labels[3])\n",
    "# print(to_categorical(gen_labels[3][0]))\n",
    "# print(y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(gen_labels.shape)\n",
    "# print(gen_images.shape)\n",
    "\n",
    "# print(gen_labels[5005])\n",
    "# plt.imshow(gen_images[5003,:,:, 0], cmap='gray')\n",
    "\n",
    "# print(classif.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
