{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D, Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# https://github.com/eriklindernoren/Keras-GAN\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "\n",
    "root = 'CGAN_results_extra/'\n",
    "\n",
    "# plotting data structure\n",
    "train_hist = {}\n",
    "train_hist['D_losses'] = []\n",
    "train_hist['G_losses'] = []\n",
    "train_hist['per_epoch_ptimes'] = []\n",
    "train_hist['total_ptime'] = []\n",
    "train_hist['accuracy'] = []\n",
    "train_hist['Model'] = []\n",
    "train_hist['classif_loss'] = []\n",
    "train_hist['classif_acc'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainClassif():\n",
    "    (train_set, train_label_raw), (_, _) = mnist.load_data()\n",
    "    train_label = to_categorical(train_label_raw)\n",
    "    train_data = np.asarray(train_set).reshape(60000, 28, 28, 1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_data, train_label, test_size=0.33, random_state=43)\n",
    "\n",
    "#     test_data = test_set.reshape(10000, 28, 28, 1)\n",
    "\n",
    "#     np.shape(train_data)\n",
    "\n",
    "    model = Sequential()\n",
    "    # model.add(Conv2D(5, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "    model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=8)\n",
    "    return model\n",
    "\n",
    "def getClassif():\n",
    "    model = Sequential()\n",
    "    # model.add(Conv2D(5, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "    model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# classif = trainClassif()\n",
    "# os.makedirs('saved_model_weights/classif/', exist_ok=True)\n",
    "# classif.save_weights('saved_model_weights/classif/pre_trained_classif_short.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classif = getClassif()\n",
    "classif.load_weights('saved_model_weights/classif/pre_trained_classif.h5', by_name=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_,_), (X_test_raw, y_test_raw) = mnist.load_data()\n",
    "X_test = X_test_raw.reshape(10000,28,28,1)\n",
    "y_test = to_categorical(y_test_raw)\n",
    "classif.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self, pick_model='Deep_BN'):\n",
    "        # Input shape\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.num_classes = 10\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator(pick_model=pick_model)\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator(pick_model=pick_model)\n",
    "\n",
    "        # The generator takes noise and the target label as input\n",
    "        # and generates the corresponding digit of that label\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([noise, label])\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
    "        valid = self.discriminator([img, label])\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model([noise, label], valid)\n",
    "        self.combined.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self, pick_model='Deep_BN'):\n",
    "        if (pick_model == 'Deep_BN') | (pick_model == 'Deeper_G_BN'):\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(256, input_dim=self.latent_dim))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(BatchNormalization(momentum=0.8))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(BatchNormalization(momentum=0.8))\n",
    "            model.add(Dense(1024))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(BatchNormalization(momentum=0.8))\n",
    "            model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "            model.add(Reshape(self.img_shape))\n",
    "\n",
    "            model.summary()\n",
    "\n",
    "        elif (pick_model == 'Shallow_BN') | (pick_model == 'Deeper_D_BN'): \n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(256, input_dim=self.latent_dim))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(BatchNormalization(momentum=0.8))\n",
    "            model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "            model.add(Reshape(self.img_shape))\n",
    "\n",
    "            model.summary()\n",
    "        elif pick_model == 'Shallow_Drop':\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(256, input_dim=self.latent_dim))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dropout(0.4))\n",
    "            model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "            model.add(Reshape(self.img_shape))\n",
    "\n",
    "            model.summary()\n",
    "        elif pick_model == 'Drop':\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(256, input_dim=self.latent_dim))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(BatchNormalization(momentum=0.8))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(BatchNormalization(momentum=0.8))\n",
    "            model.add(Dense(1024))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(BatchNormalization(momentum=0.8))\n",
    "            model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "            model.add(Reshape(self.img_shape))\n",
    "\n",
    "            model.summary()\n",
    "        elif pick_model == 'Shallow_simple':\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(128, input_dim=self.latent_dim))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "#             model.add(BatchNormalization(momentum=0.8))\n",
    "            model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "            model.add(Reshape(self.img_shape))\n",
    "\n",
    "            model.summary()\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "\n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        img = model(model_input)\n",
    "\n",
    "        return Model([noise, label], img)\n",
    "\n",
    "    def build_discriminator(self, pick_model='Deep_BN'):\n",
    "        if (pick_model == 'Deep_BN') | (pick_model == 'Deeper_D_BN'):\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(512, input_dim=np.prod(self.img_shape)))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dropout(0.4))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dropout(0.4))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.summary()\n",
    "        \n",
    "        elif (pick_model == 'Shallow_BN') | (pick_model == 'Deeper_G_BN'): \n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(512, input_dim=np.prod(self.img_shape)))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dropout(0.4))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dropout(0.4))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.summary()\n",
    "        \n",
    "        elif pick_model == 'Shallow_Drop':\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(512, input_dim=np.prod(self.img_shape)))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "            model.add(Dropout(0.4))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.summary()\n",
    "        \n",
    "        elif pick_model == 'Drop':\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(512, input_dim=np.prod(self.img_shape)))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dropout(0.4))\n",
    "            model.add(Dense(512))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dropout(0.4))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.summary()\n",
    "            \n",
    "        elif pick_model == 'Shallow_simple':\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(128, input_dim=np.prod(self.img_shape)))\n",
    "            model.add(LeakyReLU(alpha=0.2))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            model.summary()      \n",
    "            \n",
    "        img = Input(shape=self.img_shape)\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
    "        flat_img = Flatten()(img)\n",
    "\n",
    "        model_input = multiply([flat_img, label_embedding])\n",
    "\n",
    "        validity = model(model_input)\n",
    "\n",
    "        return Model([img, label], validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50, ratio = (1,1), pick_model = 'Deep_BN'):\n",
    "        best_acc = 0\n",
    "        # Load the dataset\n",
    "        (X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Configure input\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            epoch_start_time = time.time()\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs, labels = X_train[idx], y_train[idx]\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict([noise, labels])\n",
    "            #implement ratio of D  \n",
    "            if epoch % ratio[0] == 0:\n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs, labels], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, labels], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Condition on labels\n",
    "            sampled_labels = np.random.randint(0, 10, batch_size).reshape(-1, 1)\n",
    "            #implement ratio of G\n",
    "            if epoch % ratio[1] == 0:\n",
    "                # Train the generator\n",
    "                g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "            \n",
    "            # save data for plotting and csv\n",
    "            train_hist['D_losses'].append(d_loss[0])\n",
    "            train_hist['G_losses'].append(g_loss)\n",
    "            train_hist['per_epoch_ptimes'].append(per_epoch_ptime)    \n",
    "            train_hist['accuracy'].append(100 * d_loss[1])\n",
    "            train_hist['Model'].append(pick_model)\n",
    "            \n",
    "            \n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch, pick_model, ratio)\n",
    "                # Plot the progress\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "                # REMEMBER TO CHANGE TO PARAM DIRECTORY\n",
    "                if pick_model == 'Deep_BN':\n",
    "                    gen_images, gen_labels = cgan.generate_images(100)\n",
    "                    loss, acc = classif.evaluate(gen_images, to_categorical(gen_labels))\n",
    "                    train_hist['classif_loss'].append(loss)\n",
    "                    train_hist['classif_acc'].append(acc)\n",
    "                    \n",
    "                    if (acc > 0.925) & (acc > best_acc):\n",
    "                        best_acc = acc\n",
    "                        \n",
    "                        gen_images, gen_labels = cgan.generate_images(10000)\n",
    "                        image_out = gen_images.reshape(100000,784)\n",
    "\n",
    "                        df = pd.DataFrame(image_out)\n",
    "                        df.to_csv('generated_images_%d'%epoch +'_%f.csv'%acc, index=False, header=False)\n",
    "                        dfl = pd.DataFrame(gen_labels)\n",
    "                        dfl.to_csv('generated_labels_%d'%epoch +'_%f.csv'%acc, index=False, header=False)\n",
    "#                 os.makedirs('saved_model_weights/generator_weights/', exist_ok=True)\n",
    "#                 generator.save_weights('saved_model_weights/generator_weights/' + pick_model + '_' + ratio + '_' + epoch + '.h5')\n",
    "\n",
    "#             if epoch == range(epochs):    \n",
    "#                 os.makedirs('saved_model_weights/generator_weights/', exist_ok=True)\n",
    "#                 generator.save_weights('saved_model_weights/generator_weights/' + pick_model + '_' + ratio + '_' + epoch + '.h5')\n",
    "\n",
    "#                 os.makedirs('saved_model_weights', exist_ok=True)\n",
    "#                 self.generator.save_weights('saved_model_weights/generator_weights_%d.h5'%epoch)\n",
    "#                 discriminator.save_weights('saved_model_weights/discriminator_weights_%d.h5'%epoch)\n",
    "#                 combined.save_weights('saved_model_weights/combined_weights-%d.h5'%epoch)\n",
    "\n",
    "    def sample_images(self, epoch, pick_model, ratio):\n",
    "        r, c = 2, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        sampled_labels = np.arange(0, 10).reshape(-1, 1)\n",
    "\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
    "                axs[i,j].set_title(\"Digit: %d\" % sampled_labels[cnt])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        os.makedirs(root + 'images/'+  pick_model + '/'+ str(ratio[0]) + '_' + str(ratio[1]) + '/' , exist_ok=True)\n",
    "        fig.savefig(root + 'images/'+  pick_model + '/'+ str(ratio[0]) + '_' + str(ratio[1]) + '/' + str(epoch) + '.png')\n",
    "#         fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()\n",
    "        \n",
    "    def generate_images(self, num):\n",
    "        r, c = 2, 5\n",
    "        arr_imgs = []\n",
    "        arr_labels = []\n",
    "        for i in range(0, num):\n",
    "            noise = np.random.normal(0, 1, (2 * 5, 100))\n",
    "            sampled_labels = np.arange(0, 10).reshape(-1, 1)\n",
    "            gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "\n",
    "            # Rescale images 0 - 1\n",
    "            gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "            gen_images = ((gen_imgs.astype(np.float32) * 255)) \n",
    "\n",
    "            arr_imgs.append(gen_images)\n",
    "            arr_labels.append(sampled_labels)\n",
    "            \n",
    "        out_imgs = np.asarray(arr_imgs).reshape(num*10, 28, 28, 1)\n",
    "        out_labels = np.asarray(arr_labels).reshape(num*10, 1)\n",
    "        \n",
    "        return out_imgs, out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
    "    x = range(len(hist['D_losses']))\n",
    "    \n",
    "    y1 = hist['D_losses']\n",
    "    y2 = hist['G_losses']\n",
    "\n",
    "    plt.plot(x, y1, label='D_loss')\n",
    "    plt.plot(x, y2, label='G_loss')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    if save:\n",
    "        \n",
    "        plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratio_array = [ (1,1), (1,2), (1,3),  ]\n",
    "# ratio_array = [ (1,1),(2,1), (1,2), (3,1), (1,3), (5,1), (1,5), (1,10), (10,1)]\n",
    "\n",
    "# model_array = ['Shallow_simple', 'Shallow_BN'  ]\n",
    "model_array = ['Deep_BN' ]\n",
    "\n",
    "# model_array = ['Deep_BN' 'Deeper_G_BN']\n",
    "\n",
    "\n",
    "for pick_model in model_array:\n",
    "    if pick_model != model_array[0]:\n",
    "        del cgan\n",
    "    #---------------------------COMPILE SELECTED MODELS--------------------------------------#\n",
    "    #----------------------------------------LOOP OVER RATIOS--------------------------------#\n",
    "    for ratio in ratio_array:\n",
    "        if ratio != ratio_array[0]:\n",
    "            del cgan\n",
    "            \n",
    "        cgan = CGAN(pick_model=pick_model)\n",
    "\n",
    "         #----------------------------------------EXECUTION-----------------------------------#\n",
    "        start = time.time()\n",
    "\n",
    "        cgan.train(epochs=50001, batch_size=64, sample_interval=100, ratio=ratio, pick_model=pick_model) ## ratio G:D\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        elapsed_train_time = 'elapsed training time: {} min, {} sec '.format(int((end - start) / 60),\n",
    "                                                                             int((end - start) % 60))\n",
    "        train_hist['total_ptime'].append(elapsed_train_time)\n",
    "\n",
    "        print(elapsed_train_time)\n",
    "        os.makedirs(root + 'hist/', exist_ok=True)  \n",
    "        show_train_hist(train_hist, save=True, path=root + 'hist/' + str(ratio[0]) + '_' + str(ratio[1]) + pick_model +'.png')\n",
    "        # save hist data to csv\n",
    "        gen_images, gen_labels = cgan.generate_images(1000)\n",
    "        loss, acc = classif.evaluate(gen_images, to_categorical(gen_labels))\n",
    "        train_hist['classif_loss'].append(loss)\n",
    "        train_hist['classif_acc'].append(acc)\n",
    "        \n",
    "        os.makedirs(root + 'hist_csv/', exist_ok=True) \n",
    "        with open(root+ 'hist_csv/' + str(ratio[0]) + '_' + str(ratio[1]) + pick_model +'.csv', 'w') as f:\n",
    "            for key in train_hist.keys():\n",
    "                f.write(\"%s,%s\\n\"%(key,train_hist[key]))\n",
    "        \n",
    "        \n",
    "        # remove old data structure\n",
    "        del train_hist\n",
    "        # redefine\n",
    "        train_hist = {}\n",
    "        train_hist['D_losses'] = []\n",
    "        train_hist['G_losses'] = []\n",
    "        train_hist['per_epoch_ptimes'] = []\n",
    "        train_hist['total_ptime'] = []\n",
    "        train_hist['accuracy'] = []\n",
    "        train_hist['Model'] = []\n",
    "        train_hist['classif_loss'] = []\n",
    "        train_hist['classif_acc'] = []\n",
    "        gen_images, gen_labels = cgan.generate_images(1000)\n",
    "\n",
    "        loss, acc = classif.evaluate(gen_images, to_categorical(gen_labels))\n",
    "        \n",
    "#         os.makedirs('saved_model_weights/generator_weights/', exist_ok=True)\n",
    "#         os.makedirs('saved_model_weights/discriminator_weights/', exist_ok=True)\n",
    "#         os.makedirs('saved_model_weights/combined_weights/', exist_ok=True)\n",
    "#         generator.save_weights('saved_model_weights/generator_weights/' + pick_model + '_' + ratio + '_' + epoch + '.h5')\n",
    "#         discriminator.save_weights('saved_model_weights/discriminator_weights/' + pick_model + '_' + ratio + '_' + epoch + '.h5')\n",
    "#         combined.save_weights('saved_model_weights/combined_weights-%d.h5' + pick_model + '_' + ratio + '_' + epoch + '.h5')\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gen_images.shape)\n",
    "# image_out = gen_images.reshape(10000,784)\n",
    "# print(image_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(image_out)\n",
    "# df.to_csv('generated_images.csv',index=False, header=False)\n",
    "# dfl = pd.DataFrame(gen_labels)\n",
    "# dfl.to_csv('generated_labels.csv',index=False, header=False)\n",
    "# epoch = 0\n",
    "# acc = 0.5\n",
    "# gen_images, gen_labels = cgan.generate_images(10000)\n",
    "# image_out = gen_images.reshape(100000,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(image_out)\n",
    "# df.to_csv('generated_images_%d'%epoch +'_%f.csv'%acc, index=False, header=False)\n",
    "# dfl = pd.DataFrame(gen_labels)\n",
    "# dfl.to_csv('generated_labels_%d'%epoch +'_%f.csv'%acc, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_images_raw, gen_labels = cgan.generate_images(1000)\n",
    "# # gen_imgs = 0.5 * gen_images + 0.5\n",
    "# gen_images = ((gen_images_raw.astype(np.float32) * 255)) \n",
    "\n",
    "# gen_labels_cat = to_categorical(gen_labels)\n",
    "# loss, acc = classif.evaluate(gen_images, gen_labels_cat)\n",
    "# print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gen_images[0])\n",
    "# print(X_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gen_labels_cat[9])\n",
    "# print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classif.evaluate(X_test[0:5], y_test[0:5]))\n",
    "# print(classif.predict(X_test[0:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(loss)\n",
    "# print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_test.shape)\n",
    "# print(y_test.shape)\n",
    "# print(gen_images.shape)\n",
    "# print(to_categorical(gen_labels).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(to_categorical(gen_labels))\n",
    "# print(classif.predict(X_test))\n",
    "# to_categorical(gen_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gen_labels[3])\n",
    "# print(to_categorical(gen_labels[3][0]))\n",
    "# print(y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(gen_labels.shape)\n",
    "# print(gen_images.shape)\n",
    "\n",
    "# print(gen_labels[5005])\n",
    "# plt.imshow(gen_images[5003,:,:, 0], cmap='gray')\n",
    "\n",
    "# print(classif.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "venv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
