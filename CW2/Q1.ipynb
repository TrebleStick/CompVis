{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://www.kaggle.com/vincentman0403/dcgan-on-mnist \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "38443112fd55bef54d7807495b69d66353bfccae"
   },
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100\n",
    "root = 'MNIST_DCGAN_results/'\n",
    "model = 'MNIST_DCGAN_'\n",
    "\n",
    "# plotting data structure\n",
    "train_hist = {}\n",
    "train_hist['D_losses'] = []\n",
    "train_hist['G_losses'] = []\n",
    "train_hist['per_epoch_ptimes'] = []\n",
    "train_hist['total_ptime'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "804d0ab8433b786150c48cd6bd990d74705cb2a8"
   },
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=latent_dim))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(channels, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    model.summary()\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    img = model(noise)\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a9ea4efecf7d4faeb0a37a649c079ebe88396908"
   },
   "source": [
    "## Define a function to build a discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "b9aa34a385dac406b50e7e122b45483c98c0cffa"
   },
   "outputs": [],
   "source": [
    "def build_discriminator(pick_model=0):\n",
    "    if pick_model == 0:\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "    \n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "306fda1d4980efc3c64db997111ab7e615cfdbac"
   },
   "source": [
    "## Build GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "f7202cc45aae945674176146922493812e520030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jack\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Jack\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 392,833\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 855,809\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# build discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# build generator\n",
    "generator = build_generator()\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains the generator to fool the discriminator\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1d9927bd8601eedf144b6f46f12cc4da0d045b5"
   },
   "source": [
    "## Define a function to train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "6592840f1e019f1d6fd3e709a0885389d252164f"
   },
   "outputs": [],
   "source": [
    "def train(epochs, batch_size=128, save_interval=50, ratio = (1,1)): ## ratio G:D\n",
    "    os.makedirs('images', exist_ok=True)\n",
    "    \n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = X_train / 127.5 - 1.\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Select a random real images\n",
    "        epoch_start_time = time.time()\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        real_imgs = X_train[idx]\n",
    "\n",
    "        # Sample noise and generate a batch of fake images\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        fake_imgs = generator.predict(noise)\n",
    "        \n",
    "      #implement ratio of D  \n",
    "        if epoch % ratio[0] == 0:\n",
    "            # Train the discriminator\n",
    "            D_loss_real = discriminator.train_on_batch(real_imgs, valid)\n",
    "            D_loss_fake = discriminator.train_on_batch(fake_imgs, fake)\n",
    "            D_loss = 0.5 * np.add(D_loss_real, D_loss_fake)\n",
    "      #implement ratio of G\n",
    "        if epoch % ratio[1] == 0:\n",
    "            # Train the generator\n",
    "            g_loss = combined.train_on_batch(noise, valid)\n",
    "            \n",
    "        # get epoch timing\n",
    "        epoch_end_time = time.time()\n",
    "        per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "            \n",
    "        # save data for plotting\n",
    "        train_hist['D_losses'].append(D_loss[0])\n",
    "        train_hist['G_losses'].append(g_loss)\n",
    "        train_hist['per_epoch_ptimes'].append(per_epoch_ptime)    \n",
    "        \n",
    "        # If at save interval\n",
    "        if epoch % save_interval == 0:\n",
    "            # Print the progress\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f] [epoch time: %.2f]\" % (epoch, D_loss[0], 100 * D_loss[1], g_loss, per_epoch_ptime))\n",
    "            # Save generated image samples\n",
    "            save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b03c0413ddcde5125fa8fdb22bfe47e1bd668606"
   },
   "outputs": [],
   "source": [
    "def save_imgs(epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
    "    x = range(len(hist['D_losses']))\n",
    "\n",
    "    y1 = hist['D_losses']\n",
    "    y2 = hist['G_losses']\n",
    "\n",
    "    plt.plot(x, y1, label='D_loss')\n",
    "    plt.plot(x, y2, label='G_loss')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ccbe0a280b9fc16e47a63dc5e5edae8909f7569c"
   },
   "source": [
    "## Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "8a52d992f11a034e0cc0cabaddeb2ae93f160dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jack\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack\\venv\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.208578, acc.: 29.69%] [G loss: 0.662138] [epoch time: 13.01]\n",
      "1 [D loss: 0.637409, acc.: 64.06%] [G loss: 0.778305] [epoch time: 0.14]\n",
      "2 [D loss: 0.635397, acc.: 64.06%] [G loss: 1.101668] [epoch time: 0.14]\n",
      "3 [D loss: 0.450472, acc.: 78.12%] [G loss: 1.132891] [epoch time: 0.14]\n",
      "4 [D loss: 0.258223, acc.: 95.31%] [G loss: 1.218686] [epoch time: 0.14]\n",
      "5 [D loss: 0.219270, acc.: 96.88%] [G loss: 1.011388] [epoch time: 0.14]\n",
      "6 [D loss: 0.243134, acc.: 96.88%] [G loss: 1.058210] [epoch time: 0.14]\n",
      "7 [D loss: 0.115336, acc.: 100.00%] [G loss: 0.523877] [epoch time: 0.14]\n",
      "8 [D loss: 0.211301, acc.: 93.75%] [G loss: 0.888975] [epoch time: 0.14]\n",
      "9 [D loss: 0.354175, acc.: 81.25%] [G loss: 1.029729] [epoch time: 0.14]\n",
      "10 [D loss: 0.789673, acc.: 59.38%] [G loss: 1.687781] [epoch time: 0.14]\n",
      "11 [D loss: 0.740547, acc.: 57.81%] [G loss: 2.329051] [epoch time: 0.14]\n",
      "12 [D loss: 0.952504, acc.: 51.56%] [G loss: 1.792017] [epoch time: 0.14]\n",
      "13 [D loss: 0.752575, acc.: 57.81%] [G loss: 1.339215] [epoch time: 0.13]\n",
      "14 [D loss: 0.430578, acc.: 81.25%] [G loss: 1.483434] [epoch time: 0.14]\n",
      "15 [D loss: 0.347064, acc.: 85.94%] [G loss: 1.226693] [epoch time: 0.14]\n",
      "16 [D loss: 0.721214, acc.: 68.75%] [G loss: 1.337760] [epoch time: 0.14]\n",
      "17 [D loss: 0.725715, acc.: 62.50%] [G loss: 1.982563] [epoch time: 0.14]\n",
      "18 [D loss: 0.862966, acc.: 53.12%] [G loss: 1.470223] [epoch time: 0.13]\n",
      "19 [D loss: 0.495150, acc.: 73.44%] [G loss: 0.583726] [epoch time: 0.14]\n",
      "20 [D loss: 0.155977, acc.: 96.88%] [G loss: 0.343120] [epoch time: 0.14]\n",
      "21 [D loss: 0.117708, acc.: 100.00%] [G loss: 0.174591] [epoch time: 0.13]\n",
      "22 [D loss: 0.170646, acc.: 95.31%] [G loss: 0.327187] [epoch time: 0.14]\n",
      "23 [D loss: 0.217649, acc.: 93.75%] [G loss: 0.272847] [epoch time: 0.14]\n",
      "24 [D loss: 0.223547, acc.: 90.62%] [G loss: 0.507048] [epoch time: 0.14]\n",
      "25 [D loss: 0.719176, acc.: 64.06%] [G loss: 1.467141] [epoch time: 0.13]\n",
      "26 [D loss: 0.980519, acc.: 43.75%] [G loss: 1.845667] [epoch time: 0.14]\n",
      "27 [D loss: 1.473879, acc.: 26.56%] [G loss: 1.412101] [epoch time: 0.15]\n",
      "28 [D loss: 0.710166, acc.: 59.38%] [G loss: 1.460304] [epoch time: 0.14]\n",
      "29 [D loss: 0.642115, acc.: 62.50%] [G loss: 1.235165] [epoch time: 0.13]\n",
      "30 [D loss: 0.710747, acc.: 65.62%] [G loss: 1.086073] [epoch time: 0.14]\n",
      "31 [D loss: 1.037501, acc.: 45.31%] [G loss: 1.415073] [epoch time: 0.14]\n",
      "32 [D loss: 1.198030, acc.: 34.38%] [G loss: 2.003590] [epoch time: 0.13]\n",
      "33 [D loss: 1.140814, acc.: 39.06%] [G loss: 1.651336] [epoch time: 0.14]\n",
      "34 [D loss: 0.983695, acc.: 40.62%] [G loss: 1.521971] [epoch time: 0.14]\n",
      "35 [D loss: 0.692510, acc.: 62.50%] [G loss: 1.461410] [epoch time: 0.14]\n",
      "36 [D loss: 1.007514, acc.: 42.19%] [G loss: 1.417414] [epoch time: 0.14]\n",
      "37 [D loss: 1.065582, acc.: 39.06%] [G loss: 1.469720] [epoch time: 0.14]\n",
      "38 [D loss: 0.903110, acc.: 43.75%] [G loss: 1.413867] [epoch time: 0.14]\n",
      "39 [D loss: 0.672960, acc.: 60.94%] [G loss: 0.829380] [epoch time: 0.14]\n",
      "40 [D loss: 0.716323, acc.: 64.06%] [G loss: 0.769159] [epoch time: 0.14]\n",
      "41 [D loss: 0.484375, acc.: 78.12%] [G loss: 1.184494] [epoch time: 0.14]\n",
      "42 [D loss: 0.877235, acc.: 46.88%] [G loss: 1.259692] [epoch time: 0.14]\n",
      "43 [D loss: 1.084524, acc.: 35.94%] [G loss: 1.212223] [epoch time: 0.14]\n",
      "44 [D loss: 0.827175, acc.: 48.44%] [G loss: 1.524699] [epoch time: 0.14]\n",
      "45 [D loss: 0.951848, acc.: 46.88%] [G loss: 1.505717] [epoch time: 0.14]\n",
      "46 [D loss: 0.821088, acc.: 51.56%] [G loss: 1.783771] [epoch time: 0.14]\n",
      "47 [D loss: 0.859828, acc.: 48.44%] [G loss: 1.656133] [epoch time: 0.14]\n",
      "48 [D loss: 0.854356, acc.: 57.81%] [G loss: 1.664863] [epoch time: 0.14]\n",
      "49 [D loss: 1.023376, acc.: 39.06%] [G loss: 1.349725] [epoch time: 0.14]\n",
      "50 [D loss: 0.851054, acc.: 48.44%] [G loss: 1.238878] [epoch time: 0.13]\n",
      "51 [D loss: 0.777576, acc.: 56.25%] [G loss: 0.986330] [epoch time: 0.14]\n",
      "52 [D loss: 0.743001, acc.: 65.62%] [G loss: 0.922957] [epoch time: 0.14]\n",
      "53 [D loss: 0.924134, acc.: 48.44%] [G loss: 1.012061] [epoch time: 0.14]\n",
      "54 [D loss: 1.140675, acc.: 35.94%] [G loss: 1.135858] [epoch time: 0.14]\n",
      "55 [D loss: 0.863733, acc.: 56.25%] [G loss: 1.360553] [epoch time: 0.14]\n",
      "56 [D loss: 0.576342, acc.: 70.31%] [G loss: 1.668866] [epoch time: 0.14]\n",
      "57 [D loss: 0.921949, acc.: 45.31%] [G loss: 1.345481] [epoch time: 0.14]\n",
      "58 [D loss: 0.793783, acc.: 54.69%] [G loss: 1.171232] [epoch time: 0.14]\n",
      "59 [D loss: 0.759834, acc.: 54.69%] [G loss: 1.282011] [epoch time: 0.13]\n",
      "60 [D loss: 1.038203, acc.: 32.81%] [G loss: 1.113146] [epoch time: 0.13]\n",
      "61 [D loss: 0.799934, acc.: 56.25%] [G loss: 1.389507] [epoch time: 0.13]\n",
      "62 [D loss: 0.869619, acc.: 48.44%] [G loss: 1.387820] [epoch time: 0.13]\n",
      "63 [D loss: 0.855639, acc.: 50.00%] [G loss: 1.456386] [epoch time: 0.14]\n",
      "64 [D loss: 0.902036, acc.: 53.12%] [G loss: 1.286313] [epoch time: 0.14]\n",
      "65 [D loss: 0.938410, acc.: 42.19%] [G loss: 1.341349] [epoch time: 0.13]\n",
      "66 [D loss: 0.970449, acc.: 46.88%] [G loss: 1.245054] [epoch time: 0.13]\n",
      "67 [D loss: 0.769929, acc.: 54.69%] [G loss: 1.508092] [epoch time: 0.13]\n",
      "68 [D loss: 0.868044, acc.: 45.31%] [G loss: 1.534425] [epoch time: 0.13]\n",
      "69 [D loss: 0.785289, acc.: 51.56%] [G loss: 1.221011] [epoch time: 0.14]\n",
      "70 [D loss: 0.865799, acc.: 43.75%] [G loss: 1.259564] [epoch time: 0.13]\n",
      "71 [D loss: 0.975034, acc.: 37.50%] [G loss: 1.043816] [epoch time: 0.13]\n",
      "72 [D loss: 0.847433, acc.: 45.31%] [G loss: 1.085736] [epoch time: 0.13]\n",
      "73 [D loss: 0.941407, acc.: 46.88%] [G loss: 1.301291] [epoch time: 0.13]\n",
      "74 [D loss: 0.866398, acc.: 46.88%] [G loss: 1.166805] [epoch time: 0.13]\n",
      "75 [D loss: 0.793988, acc.: 56.25%] [G loss: 1.341701] [epoch time: 0.13]\n",
      "76 [D loss: 1.039598, acc.: 31.25%] [G loss: 1.336036] [epoch time: 0.13]\n",
      "77 [D loss: 1.004468, acc.: 42.19%] [G loss: 1.097772] [epoch time: 0.13]\n",
      "78 [D loss: 0.845630, acc.: 48.44%] [G loss: 1.386403] [epoch time: 0.14]\n",
      "79 [D loss: 0.723132, acc.: 59.38%] [G loss: 1.326344] [epoch time: 0.14]\n",
      "80 [D loss: 0.725326, acc.: 59.38%] [G loss: 1.075061] [epoch time: 0.13]\n",
      "81 [D loss: 0.868038, acc.: 48.44%] [G loss: 1.073534] [epoch time: 0.13]\n",
      "82 [D loss: 0.910886, acc.: 39.06%] [G loss: 1.080282] [epoch time: 0.14]\n",
      "83 [D loss: 0.837981, acc.: 40.62%] [G loss: 1.390333] [epoch time: 0.14]\n",
      "84 [D loss: 0.701790, acc.: 57.81%] [G loss: 1.172152] [epoch time: 0.13]\n",
      "85 [D loss: 0.949365, acc.: 40.62%] [G loss: 1.056278] [epoch time: 0.14]\n",
      "86 [D loss: 0.740194, acc.: 56.25%] [G loss: 0.973564] [epoch time: 0.13]\n",
      "87 [D loss: 0.938187, acc.: 45.31%] [G loss: 1.018990] [epoch time: 0.14]\n",
      "88 [D loss: 0.753016, acc.: 56.25%] [G loss: 1.216699] [epoch time: 0.13]\n",
      "89 [D loss: 0.869718, acc.: 39.06%] [G loss: 1.307332] [epoch time: 0.14]\n",
      "90 [D loss: 0.704217, acc.: 60.94%] [G loss: 1.170099] [epoch time: 0.14]\n",
      "91 [D loss: 0.940331, acc.: 40.62%] [G loss: 1.219333] [epoch time: 0.13]\n",
      "92 [D loss: 0.849800, acc.: 48.44%] [G loss: 0.961621] [epoch time: 0.14]\n",
      "93 [D loss: 0.698440, acc.: 57.81%] [G loss: 1.240580] [epoch time: 0.14]\n",
      "94 [D loss: 0.833693, acc.: 40.62%] [G loss: 0.965863] [epoch time: 0.14]\n",
      "95 [D loss: 0.743232, acc.: 57.81%] [G loss: 1.056797] [epoch time: 0.14]\n",
      "96 [D loss: 0.807669, acc.: 45.31%] [G loss: 1.233903] [epoch time: 0.14]\n",
      "97 [D loss: 0.925551, acc.: 35.94%] [G loss: 1.355981] [epoch time: 0.14]\n",
      "98 [D loss: 0.877577, acc.: 46.88%] [G loss: 0.994171] [epoch time: 0.14]\n",
      "99 [D loss: 0.969222, acc.: 50.00%] [G loss: 1.398372] [epoch time: 0.14]\n",
      "elapsed training time: 1 min, 26 sec \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'show_train_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f586d270895b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melapsed_train_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mshow_train_hist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mroot\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'train_hist.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'show_train_hist' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# train_hist = {}\n",
    "#     train_hist['D_losses'] = []\n",
    "#     train_hist['G_losses'] = []\n",
    "#     train_hist['per_epoch_ptimes'] = []\n",
    "#     train_hist['total_ptime'] = []\n",
    "\n",
    "train(epochs=100, batch_size=32, save_interval=1, ratio=(1,1)) ## ratio G:D\n",
    "\n",
    "end = time.time()\n",
    "elapsed_train_time = 'elapsed training time: {} min, {} sec '.format(int((end - start) / 60),\n",
    "                                                                     int((end - start) % 60))\n",
    "train_hist['total_ptime'].append(elapsed_train_time)\n",
    "\n",
    "print(elapsed_train_time)\n",
    "\n",
    "show_train_hist(train_hist, save=True, path=root + model + 'train_hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cb7c9e65347eb077e1dabd74445713ddef12ba34"
   },
   "outputs": [],
   "source": [
    "os.makedirs('saved_model_weights', exist_ok=True)\n",
    "generator.save_weights('saved_model_weights/generator_weights.h5')\n",
    "discriminator.save_weights('saved_model_weights/discriminator_weights.h5')\n",
    "combined.save_weights('saved_model_weights/combined_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "526455382f869ceadc4206cb703e676a4037b213",
    "collapsed": true
   },
   "source": [
    "## Show generated MNIST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9cd883f622680511e19cfa30717a3a8d93c4a48"
   },
   "outputs": [],
   "source": [
    "Image.open('images/mnist_1000.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c487c7fede8db3baa681554c9cf677464af791c"
   },
   "outputs": [],
   "source": [
    "Image.open('images/mnist_9000.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1a1fdfff62338dd332aa823655f398156a5e5ae"
   },
   "source": [
    "## Reference\n",
    "[Keras - DCGAN](https://github.com/eriklindernoren/Keras-GAN#dcgan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
