{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "38443112fd55bef54d7807495b69d66353bfccae"
   },
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "804d0ab8433b786150c48cd6bd990d74705cb2a8"
   },
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=latent_dim))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(channels, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    model.summary()\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    img = model(noise)\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a9ea4efecf7d4faeb0a37a649c079ebe88396908"
   },
   "source": [
    "## Define a function to build a discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "b9aa34a385dac406b50e7e122b45483c98c0cffa"
   },
   "outputs": [],
   "source": [
    "def build_discriminator(pick_model=0):\n",
    "    if pick_model == 0:\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "    \n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "306fda1d4980efc3c64db997111ab7e615cfdbac"
   },
   "source": [
    "## Build GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7202cc45aae945674176146922493812e520030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jack\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Jack\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 392,833\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 855,809\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# build discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# build generator\n",
    "generator = build_generator()\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains the generator to fool the discriminator\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1d9927bd8601eedf144b6f46f12cc4da0d045b5"
   },
   "source": [
    "## Define a function to train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6592840f1e019f1d6fd3e709a0885389d252164f"
   },
   "outputs": [],
   "source": [
    "def train(epochs, batch_size=128, save_interval=50, ratio = (1,1)): ## ratio G:D\n",
    "    os.makedirs('images', exist_ok=True)\n",
    "    \n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = X_train / 127.5 - 1.\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Select a random real images\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        real_imgs = X_train[idx]\n",
    "\n",
    "        # Sample noise and generate a batch of fake images\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        fake_imgs = generator.predict(noise)\n",
    "        \n",
    "      #implement ratio of D  \n",
    "        if epoch % ratio[0] == 0:\n",
    "            # Train the discriminator\n",
    "            D_loss_real = discriminator.train_on_batch(real_imgs, valid)\n",
    "            D_loss_fake = discriminator.train_on_batch(fake_imgs, fake)\n",
    "            D_loss = 0.5 * np.add(D_loss_real, D_loss_fake)\n",
    "      #implement ratio of G\n",
    "        if epoch % ratio[1] == 0:\n",
    "            # Train the generator\n",
    "            g_loss = combined.train_on_batch(noise, valid)\n",
    "\n",
    "        # If at save interval\n",
    "        if epoch % save_interval == 0:\n",
    "            # Print the progress\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, D_loss[0], 100 * D_loss[1], g_loss))\n",
    "            # Save generated image samples\n",
    "            save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b03c0413ddcde5125fa8fdb22bfe47e1bd668606"
   },
   "outputs": [],
   "source": [
    "def save_imgs(epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ccbe0a280b9fc16e47a63dc5e5edae8909f7569c"
   },
   "source": [
    "## Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8a52d992f11a034e0cc0cabaddeb2ae93f160dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jack\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack\\venv\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.098835, acc.: 42.19%] [G loss: 1.360377]\n",
      "100 [D loss: 2.099577, acc.: 37.50%] [G loss: 1.754986]\n",
      "200 [D loss: 2.619287, acc.: 10.94%] [G loss: 3.472445]\n",
      "300 [D loss: 3.219313, acc.: 0.00%] [G loss: 3.589652]\n",
      "400 [D loss: 2.899303, acc.: 4.69%] [G loss: 2.417928]\n",
      "500 [D loss: 2.999387, acc.: 4.69%] [G loss: 3.231985]\n",
      "600 [D loss: 2.953775, acc.: 0.00%] [G loss: 4.722473]\n",
      "700 [D loss: 3.419774, acc.: 0.00%] [G loss: 3.321720]\n",
      "800 [D loss: 2.743527, acc.: 1.56%] [G loss: 2.485560]\n",
      "900 [D loss: 2.801949, acc.: 1.56%] [G loss: 2.892953]\n",
      "1000 [D loss: 3.070115, acc.: 1.56%] [G loss: 2.488469]\n",
      "1100 [D loss: 2.412838, acc.: 0.00%] [G loss: 3.227512]\n",
      "1200 [D loss: 2.207310, acc.: 3.12%] [G loss: 2.848693]\n",
      "1300 [D loss: 2.562399, acc.: 1.56%] [G loss: 2.508059]\n",
      "1400 [D loss: 2.398129, acc.: 0.00%] [G loss: 2.733413]\n",
      "1500 [D loss: 2.465316, acc.: 0.00%] [G loss: 2.688105]\n",
      "1600 [D loss: 2.081047, acc.: 1.56%] [G loss: 2.297460]\n",
      "1700 [D loss: 2.176364, acc.: 0.00%] [G loss: 2.004865]\n",
      "1800 [D loss: 2.207097, acc.: 1.56%] [G loss: 1.950608]\n",
      "1900 [D loss: 1.980390, acc.: 3.12%] [G loss: 2.108718]\n",
      "2000 [D loss: 1.959478, acc.: 3.12%] [G loss: 2.075203]\n",
      "2100 [D loss: 2.152753, acc.: 0.00%] [G loss: 2.161846]\n",
      "2200 [D loss: 1.832126, acc.: 1.56%] [G loss: 1.880077]\n",
      "2300 [D loss: 1.998339, acc.: 1.56%] [G loss: 1.595278]\n",
      "2400 [D loss: 1.595753, acc.: 9.38%] [G loss: 1.659312]\n",
      "2500 [D loss: 1.817608, acc.: 0.00%] [G loss: 1.581510]\n",
      "2600 [D loss: 1.518313, acc.: 4.69%] [G loss: 1.696566]\n",
      "2700 [D loss: 1.533295, acc.: 0.00%] [G loss: 1.518364]\n",
      "2800 [D loss: 1.699444, acc.: 1.56%] [G loss: 1.404863]\n",
      "2900 [D loss: 1.450262, acc.: 7.81%] [G loss: 1.421437]\n",
      "3000 [D loss: 1.418920, acc.: 6.25%] [G loss: 1.309300]\n",
      "3100 [D loss: 1.361501, acc.: 6.25%] [G loss: 1.513473]\n",
      "3200 [D loss: 1.218229, acc.: 9.38%] [G loss: 1.275164]\n",
      "3300 [D loss: 1.439304, acc.: 3.12%] [G loss: 1.260988]\n",
      "3400 [D loss: 1.328922, acc.: 7.81%] [G loss: 1.256945]\n",
      "3500 [D loss: 1.344938, acc.: 4.69%] [G loss: 1.267992]\n",
      "3600 [D loss: 1.360051, acc.: 4.69%] [G loss: 1.331992]\n",
      "3700 [D loss: 1.312754, acc.: 12.50%] [G loss: 1.427779]\n",
      "3800 [D loss: 1.289405, acc.: 9.38%] [G loss: 1.419490]\n",
      "3900 [D loss: 1.410206, acc.: 3.12%] [G loss: 1.059091]\n",
      "4000 [D loss: 1.458624, acc.: 4.69%] [G loss: 1.140410]\n",
      "4100 [D loss: 1.180018, acc.: 12.50%] [G loss: 1.075865]\n",
      "4200 [D loss: 1.216365, acc.: 14.06%] [G loss: 1.260829]\n",
      "4300 [D loss: 1.217384, acc.: 9.38%] [G loss: 1.557603]\n",
      "4400 [D loss: 1.327664, acc.: 9.38%] [G loss: 1.122300]\n",
      "4500 [D loss: 1.156183, acc.: 17.19%] [G loss: 1.089451]\n",
      "4600 [D loss: 1.187536, acc.: 7.81%] [G loss: 1.110553]\n",
      "4700 [D loss: 1.280393, acc.: 7.81%] [G loss: 1.331727]\n",
      "4800 [D loss: 1.199926, acc.: 4.69%] [G loss: 1.264247]\n",
      "4900 [D loss: 1.228399, acc.: 7.81%] [G loss: 1.239539]\n",
      "5000 [D loss: 1.151994, acc.: 7.81%] [G loss: 0.991556]\n",
      "5100 [D loss: 1.166320, acc.: 10.94%] [G loss: 1.043712]\n",
      "5200 [D loss: 1.206499, acc.: 7.81%] [G loss: 1.049420]\n",
      "5300 [D loss: 1.093156, acc.: 12.50%] [G loss: 1.174097]\n",
      "5400 [D loss: 1.205705, acc.: 12.50%] [G loss: 1.068235]\n",
      "5500 [D loss: 0.972147, acc.: 28.12%] [G loss: 1.216339]\n",
      "5600 [D loss: 1.169989, acc.: 7.81%] [G loss: 1.088496]\n",
      "5700 [D loss: 1.067337, acc.: 14.06%] [G loss: 1.123315]\n",
      "5800 [D loss: 1.181526, acc.: 12.50%] [G loss: 0.984450]\n",
      "5900 [D loss: 1.063987, acc.: 15.62%] [G loss: 1.146309]\n",
      "6000 [D loss: 1.232363, acc.: 6.25%] [G loss: 1.068500]\n",
      "6100 [D loss: 1.118268, acc.: 7.81%] [G loss: 1.167073]\n",
      "6200 [D loss: 1.181049, acc.: 7.81%] [G loss: 0.982326]\n",
      "6300 [D loss: 0.946882, acc.: 23.44%] [G loss: 1.120051]\n",
      "6400 [D loss: 1.109611, acc.: 7.81%] [G loss: 1.070192]\n",
      "6500 [D loss: 1.084957, acc.: 10.94%] [G loss: 1.217829]\n",
      "6600 [D loss: 1.117634, acc.: 12.50%] [G loss: 1.002277]\n",
      "6700 [D loss: 1.083383, acc.: 6.25%] [G loss: 1.033168]\n",
      "6800 [D loss: 1.091695, acc.: 17.19%] [G loss: 0.863293]\n",
      "6900 [D loss: 1.086053, acc.: 7.81%] [G loss: 1.011271]\n",
      "7000 [D loss: 1.089732, acc.: 18.75%] [G loss: 1.032437]\n",
      "7100 [D loss: 1.014624, acc.: 25.00%] [G loss: 0.892105]\n",
      "7200 [D loss: 1.187980, acc.: 7.81%] [G loss: 1.036069]\n",
      "7300 [D loss: 1.021248, acc.: 10.94%] [G loss: 0.935247]\n",
      "7400 [D loss: 0.922910, acc.: 25.00%] [G loss: 1.118898]\n",
      "7500 [D loss: 1.191511, acc.: 7.81%] [G loss: 0.923263]\n",
      "7600 [D loss: 0.994863, acc.: 17.19%] [G loss: 0.985167]\n",
      "7700 [D loss: 1.155231, acc.: 12.50%] [G loss: 1.097730]\n",
      "7800 [D loss: 0.963550, acc.: 17.19%] [G loss: 0.924830]\n",
      "7900 [D loss: 1.030378, acc.: 15.62%] [G loss: 0.924402]\n",
      "8000 [D loss: 1.108191, acc.: 7.81%] [G loss: 0.870555]\n",
      "8100 [D loss: 1.024534, acc.: 15.62%] [G loss: 0.976147]\n",
      "8200 [D loss: 0.952854, acc.: 28.12%] [G loss: 0.929363]\n",
      "8300 [D loss: 0.912747, acc.: 26.56%] [G loss: 1.071607]\n",
      "8400 [D loss: 1.011812, acc.: 21.88%] [G loss: 1.079011]\n",
      "8500 [D loss: 0.976705, acc.: 17.19%] [G loss: 0.959100]\n",
      "8600 [D loss: 1.033227, acc.: 9.38%] [G loss: 0.919371]\n",
      "8700 [D loss: 1.008162, acc.: 12.50%] [G loss: 0.931032]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train(epochs=10000, batch_size=32, save_interval=100, ratio=(100,1)) ## ratio G:D\n",
    "\n",
    "end = time.time()\n",
    "elapsed_train_time = 'elapsed training time: {} min, {} sec '.format(int((end - start) / 60),\n",
    "                                                                     int((end - start) % 60))\n",
    "print(elapsed_train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cb7c9e65347eb077e1dabd74445713ddef12ba34"
   },
   "outputs": [],
   "source": [
    "os.makedirs('saved_model_weights', exist_ok=True)\n",
    "generator.save_weights('saved_model_weights/generator_weights.h5')\n",
    "discriminator.save_weights('saved_model_weights/discriminator_weights.h5')\n",
    "combined.save_weights('saved_model_weights/combined_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "526455382f869ceadc4206cb703e676a4037b213",
    "collapsed": true
   },
   "source": [
    "## Show generated MNIST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9cd883f622680511e19cfa30717a3a8d93c4a48"
   },
   "outputs": [],
   "source": [
    "Image.open('images/mnist_1000.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c487c7fede8db3baa681554c9cf677464af791c"
   },
   "outputs": [],
   "source": [
    "Image.open('images/mnist_9000.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1a1fdfff62338dd332aa823655f398156a5e5ae"
   },
   "source": [
    "## Reference\n",
    "[Keras - DCGAN](https://github.com/eriklindernoren/Keras-GAN#dcgan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
