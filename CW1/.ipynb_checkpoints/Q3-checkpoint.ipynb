{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SIFT data\n",
    "mat_te = scipy.io.loadmat('csvs/desc_te.mat')\n",
    "mat_tr = scipy.io.loadmat('csvs/desc_tr.mat')\n",
    "\n",
    "# Access arrays\n",
    "desc_tr_raw = mat_tr['desc_tr']\n",
    "desc_te_raw = mat_te['desc_te']\n",
    "\n",
    "# Reshape to column\n",
    "desc_tr = desc_tr_raw.reshape(150)\n",
    "desc_te = desc_te_raw.reshape(150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data\n",
    "data_train  = desc_tr[0].T\n",
    "class_train = np.asarray([1] * desc_tr[0].T.shape[0]) \n",
    "data_test  = desc_te[0].T\n",
    "class_test = np.asarray([1] * desc_te[0].T.shape[0]) \n",
    "\n",
    "# Create columns of SIFT patches\n",
    "for x in range(1,150):\n",
    "    data_train   = np.concatenate([data_train, desc_tr[x].T])\n",
    "    class_train  = np.concatenate([class_train, np.asarray([x%10 +1] * desc_tr[x].T.shape[0])])\n",
    "    data_test   = np.concatenate([data_test, desc_te[x].T])\n",
    "    class_test  = np.concatenate([class_test, np.asarray([x%10 +1] * desc_te[x].T.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle sift patches\n",
    "data_train_shuf = data_train\n",
    "data_test_shuf = data_test\n",
    "class_train_shuf = class_train\n",
    "class_test_shuf = class_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for numTrees in [1024] :\n",
    "    for maxDepth in [16] :\n",
    "#         with open('csvs/bigdata4.csv', 'a', newline='') as csvFile:\n",
    "#             writer = csv.writer(csvFile)\n",
    "        for maxFeatures in [\"auto\"] :\n",
    "\n",
    "            # Build Model\n",
    "            clf = RandomForestClassifier(n_estimators=numTrees, \n",
    "                         max_depth=maxDepth, \n",
    "                         max_features=maxFeatures,\n",
    "                         bootstrap=True,\n",
    "                         criterion=\"entropy\",\n",
    "                         random_state=21,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "            # Fit Model\n",
    "            clf.fit(data_train_shuf, class_train_shuf)\n",
    "\n",
    "            # Accuracy\n",
    "            predictScore = clf.score(data_test_shuf, class_test_shuf)\n",
    "            print(predictScore)\n",
    "            packet = [numBins, numTrees, maxDepth, maxFeatures, predictScore]\n",
    "            \n",
    "            print(decision_path(data_train_shuf))\n",
    "\n",
    "#                 writer.writerow(packet)\n",
    "#             csvFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for numBins in [128, 256, 512, 1024, 2048] :\n",
    "    for numTrees in [1024] :\n",
    "        for maxDepth in [32] :\n",
    "            with open('csvs/bigdata3.csv', 'a', newline='') as csvFile:\n",
    "                writer = csv.writer(csvFile)\n",
    "                for maxFeatures in range(1, numBins, (numBins//16)) :\n",
    "                    # File Names\n",
    "                    data_test_filename = 'csvs/test_data_' + str(numBins) +'.csv'\n",
    "                    data_train_filename = 'csvs/train_data_' + str(numBins) +'.csv'\n",
    "\n",
    "                    # Get csv raw data\n",
    "                    data_test_raw = pd.read_csv(data_test_filename, header=None)\n",
    "                    data_train_raw = pd.read_csv(data_train_filename, header=None)\n",
    "                    data_test = data_test_raw.values\n",
    "                    data_train = data_train_raw.values\n",
    "\n",
    "                    # Append class to the raw data\n",
    "                    class_train = np.array([])\n",
    "                    class_test  = np.array([])\n",
    "                    for j in range(10):\n",
    "                        for i in range(15):\n",
    "                            class_test  = np.append(class_test,  j+1)\n",
    "                            class_train = np.append(class_train, j+1)\n",
    "\n",
    "                    idx = np.arange(150)\n",
    "\n",
    "                    # Shuffle train data\n",
    "                    np.random.seed(21)\n",
    "                    np.random.shuffle(idx)\n",
    "                    data_train_shuf = data_train[idx]\n",
    "                    class_train_shuf = class_train[idx]\n",
    "\n",
    "                    # Shuffle test data\n",
    "                    np.random.seed(42)\n",
    "                    np.random.shuffle(idx)\n",
    "                    data_test_shuf = data_test[idx]\n",
    "                    class_test_shuf = class_test[idx]\n",
    "\n",
    "                    # Build Model\n",
    "                    clf = RandomForestClassifier(n_estimators=numTrees, \n",
    "                                 max_depth=maxDepth, \n",
    "                                 max_features=maxFeatures,\n",
    "                                 bootstrap=True,\n",
    "                                 criterion=\"entropy\",\n",
    "                                 random_state=21,\n",
    "                                 n_jobs=-1)\n",
    "\n",
    "                    # Fit Model\n",
    "                    clf.fit(data_train_shuf, class_train_shuf)\n",
    "\n",
    "                    # Accuracy\n",
    "                    predictScore = clf.score(data_test_shuf, class_test_shuf)\n",
    "\n",
    "                    # Classes Predicted\n",
    "    #                 predictions = clf.predict(data_test_shuf)\n",
    "\n",
    "                    # Probability Of Prediction - confidence\n",
    "    #                 predict_probs = clf.predict_proba(data_test_shuf)\n",
    "\n",
    "                    packet = [numBins, numTrees, maxDepth, maxFeatures, predictScore]\n",
    "\n",
    "\n",
    "                    writer.writerow(packet)\n",
    "                csvFile.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('YEEEY')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
